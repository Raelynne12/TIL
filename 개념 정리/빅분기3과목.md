## 1. 분석 절차 수립



**분석 모형 선정 필요성**

1. 의사 결정
2. 불확실성 해소
3. 요약
4. 인과관계 파악
5. 예측



**분석 모형 선정 프로세스**

1. 문제요건 정의 또는 비즈니스 이해에 따른 대상 데이터 선정과 분석 목표/조건 정의 
2. 데이터 수집, 정리 및 도식화
3. 데이터 전처리
4. 최적의 분석 모형 선정



**분석 모형 종류**

1. 예측 분석 모형
   - 어떤 일들이 발생할 것인가?
2. 현황 진단 모형
   - 과거에 어떠한 상황이 왜 어떻게 일어났는가?
3. 최적화 분석 모형
   - 어떻게 하면 원하는 결과가 일어날 수 있을까?



**분석 모형 정의 위한 사전 고려사항**

- 상향식
- 하향식
  - 비즈니스 모델
    - 어떻게 수익을 창출할 것인가
  - 외부 참조 모델
    - 벤치마킹으로 분석 테마 후보 pool을 구축, 선택
  - 분석 유즈케이스 기반 모델
    - 문제에 대한 상세 설명과 해결 시 효과에 대해 명시



**분석 모형 설계**

> 분석 대상 및 범위 정해서 분석 목적 구현하기 위한 분석방법론 설계하는 단계

1. 사전 확인 사항
2. 분석 모델링 설계와 검정
   - 가설검정 방법 수립
   - 추정방법에 대한 기술 검토
   - 분석 모델링 설계와 검정 방법 수립
3. 적합한 알고리즘 설계
   - 비지도학습 : 군집분석, 연관성분석, 오토인코더 등
   - 지도학습 : 의사결정트리, 랜덤 포레스트, 서포트벡터머신, 회귀분석 등
   - 준지도학습 : 셀프 트레이닝, 적대적 생성 모델 등
   - 강화학습 : Q-Learning, 정책경사 등
4. 개발 및 테스트



**통계적 가설검정**

> 통계적 추측의 하나
>
> 모집단의 실제값이 얼마가 된다는 주장에 대해 표본의 정보를 사용해서 가설의 합당성 여부 판정하는 과정
>
> = 가설검정
>
> 통계적 가설 : 특정 주장을 모수를 이용해서 나타낸 형태를 지칭 > 미국 성인여자의 평균 신장은 180이다
>
> 귀무가설과 대립가설로 나타남



**가설검정 5단계**

1. 유의수준 결정, 귀무가설과 대립가설 설정
   - 귀무가설 : 직접 검정이 되는 가설(영가설)(기각되는 것이 예상되어 세워진 가설)
   - 귀무가설이 옳다는 가정 하에 시작
   - 진실일 가능성이 적어서 처음부터 기각될 것이 예상되는 가설
2. 검정통계량의 설정
   - 가설을 검정하기 위한 기준으로 사용하는 값
3. 기각역의 설정
   - 귀무가설 기각하는 영역
4. 검정통계량 계산
   - (표본평균 - 모평균) / 표본 표준편차
   - 신뢰수준 : 어느 정도로 검정할 것인지에 대한 수준
   - 유의수준 : 유의수준 벗어나면 귀무가설 오류로 판단 (1 - 신뢰수준) > 알파값(α)
5. 통계적인 의사결정(가설검정)
   - 양측검정
   - 단측검정
   - 검정통계량을 t값 분포도와 비교해서 기각역에 속하는지 아닌지 판단



**분석모델링 설계와 검정**

- 분석모형
  - 예측 분석
  - 현황 진단
  - 예측 최적화
  - 종속변수 유무에 따라 사용할 알고리즘이 제한됨
    - 종속변수 없으면 군집과 원인분석, 이상치, 연관 법칙 등으로 제한
  - 변수의 속성에 따라 알고리즘 선택 달라짐



---

## 2. 분석 환경 구축



**R**

- 객체지향 언어
- 고속메모리 처리
- 대용량 메모리 처리 어려움
- 보안 기능 취약
- 별도 모듈 연동 아니면 웹 브라우저에서 사용 못 함



**파이썬**

- 인터프리터 언어
- 동적인 데이터타입 결정 지원
- 플랫폼 독립적 언어



**과대적합**

- 방지를 위해 데이터 분할과 k-fold 교차검증, 정규화 등의 방법이 있음



---



## 3. 분석기법



**학습 유형에 따른 데이터 분석 모델**

1. 지도학습 : 대표적으로 분류와 회귀로 구분
   - 분류 : 의사결정트리(분류), 랜덤 포레스트, 인공신경망(지도학습), 서포트벡터머신, 로지스틱 회귀분석
   - 회귀 : 의사결정트리(회귀), 선형회귀분석, 다중회귀분석
2. 비지도학습 
   - 예를 들어 전기자동차를 산 구매자들의 데이터를 가지고 있을 때, 어떤 사람들이 주로 전기자동차를 구매하는지 알아봄
   - 입력값은 있으나 정답이 없어서 출력값이 존재하지 않으므로 학습모델의 성능을 평가하기 어려움
   - 군집분석, 연관성분석, 인공신경망, 오토인코더
3. 준지도학습
   - 정답있는 데이터와 없는 데이터 동시에 학습
   - 데이터 라벨링을 하는 데 훈련된 사람의 손을 거쳐야 하므로 비용 크게 듦
   - 레이블 된 소수의 데이터만으로 부분학습모델을 만들고, 나머지 레이블이 없는 데이터에 레이블 생성 후 지도학습
   - 셀프 트레이닝, GAN
     - 셀프 트레이닝 : 정답있는 데이터로 학습한 뒤 정답없는 데이터로 예측해서 가장 높은 확률값 데이터만 다시 정답 데이터로 가져가고 > 이걸 반속해서 높은 확률값이 나오는 데에 가중치를 둠
     - GAN(생성적 적대 신경망) : 위조지폐범과 경찰 사이의 게임과 같음(생성모델과 판별모델로 나뉨)
4. 강화학습
   - 주어진 환경에서 보상 최대화하도록 에이전트 학습

| 비지도학습 | 군집분석, 연관성 분석, 인공신경망, 오토인코더 |
| ---------- | --------------------------------------------- |
| 준지도학습 | 셀프 트레이닝, GAN                            |
| 강화학습   | Q-Leaning, 정책경사(PG)                       |



**데이터 분석 알고리즘과 분야**

| 업리프트 모델링             | 단계적 추정, 예측 분석          |
| --------------------------- | ------------------------------- |
| 회귀분석                    | 예측, 추정                      |
| 시각화                      | 원인과 관계                     |
| 부스팅, 배깅                | 분류 분석                       |
| 시계열분석                  | 시간상의 예측(이자율)           |
| 요인분석                    | 차원축소                        |
| 텍스트 마이닝               | 감성 분석                       |
| 의사결정 나무, 랜덤포레스트 | 분류                            |
| 신경 회로망                 | 예측 분석                       |
| 군집분석                    | 독립변수들만의 분류, 그룹화     |
| 앙상블 기법                 | 추정, 예측, 규범 등의 결합 분석 |
| 서포트벡터머신              | 분류 분석                       |
| 주성분분석                  | 원인 분석, 차원축소             |

- 업리프팅 모델 > 마케팅 캠페인에서 많이 사용(추정 모델을 단계별로 적용)
- 로지스틱 회귀분석은 이진분류에 자주 활용
- 시계열 분석 > 이자율이나 주식 예측 등에 자주 사용(이상치 감지 기법)



**회귀분석**

- 독립변수 : 입력값, 원인을 설명하는 변수
- 종속변수 : 결과값, 효과를 설명하는 변수
- 회귀선(회귀계수) : 독립변수 주어질 때 종속변수의 기대값
- 최소제곱법 : 잔차 제곱의 합이 최소가 되게 하는 직선을 찾는 방법



**회귀분석 모형 진단**

1. 적합도 검정
   - 회귀식이 표본의 실제값을 얼마나 잘 설명하는지
   - R2(결정계수) > SSR/SST 
     - SSR : 회귀모형에 의해 설명되는 변동의 제곱합
     - SSE : 오차에 의해 설명되는 변동의 제곱합
     - SSR + SSE = SST
2. 변수 영향력 분석
   - 종속변수에 독립변수들이 얼만큼 영향력을 미치는 지 
   - p값이 0.05보다 작으면 통계적으로 유의미하다
3. 그 외(잔차분석 등)



**선형회귀분석**

> 종속변수와 독립변수 모두 연속형이어야

1. 단순 선형회귀분석
2. 다중 선형회귀분석

- 기본적 가정
  - 선형성
  - 잔차 정규성
  - 잔차 독립성
  - 잔차 등분산성
  - 다중 공선성



**로지스틱 회귀분석**

> 다른 것들은 선형회귀분석과 유사하지만
>
> 종속변수가 연속형이 아닌 범주형으로 주어지면, 특정 분류로 결과가 나타남

1. 단순 로지스틱 회귀분석 : 종속변수가 이항형 문제
2. 다중 로지스틱 회귀분석

- 승산으로 로짓변환을 통해 선형함수로 치환 가능
  - 승산 : P(A) / P(Ac)

- 복수의 범주이면서 순서가 존재하면 > 서수 로지스틱 회귀



**의사결정나무**

> 전체자료를 몇 개의 소집단으로 분류하거나 예측을 수행

- 종류
  - 분류나무 : 이산형(범주형) 목표변수에 따른 빈도기반 분리
    - 가지분할 진행 시 : 카이제곱 통계량의 p값, 지니지수, 엔트로피 지수 등이 분리 기준
    - 분리 기준을 선택할 때 불순도를 감소시키도록 설정해야
    - 불순도 차이 : 정보 획득
    - 카이제곱 : ((실제도수 - 기대도수)2 / 기대도수)의 합
    - 지니지수 : 특정 집합에서 한 항목을 뽑아서 무작위로 라벨 추정시 틀릴 확률 >> 1 - P(A) - P(Ac)
    - 엔트로피 지수 : 무질서 정도에 대한 측도
  - 회귀나무 : 연속형 목표변수에 따른 평균/표준편차 기반 
    - 가지분할 진행 시 : F-통계량의 p값, 분산의 감소량 등이 분리 기준
    - F 통계량의 p값 : 등분산성을 검정해서 p값이 커지면 등분산성 있음 = 낮은 이질성 = 순수도 높아짐
    - 분산의 감소량 : 최대화가 될수록 낮은 이질성 = 순수도 높아짐



**의사결정나무 분석 과정**

1. 변수 선택
2. 의사결정나무 형성 : 적절한 분리기준과 정지규칙, 평가기준 등으로 의사결정 나무 만들기
3. 가지치기 : 오버피팅 막고 일반화 성능 높임
4. 모형 평가 및 예측 : 이익, 위험, 비용 고려
   - 재귀적 분기 학습 : 분기 전보다 분기 후 정보 획득량 높아지도록 분기 반복(모든 잎 엔트로피가 0될 때까지 반복)
5. 가지치기 : 평가용 데이터 활용
   - 에러감소 가지치기 : 오류를 비교해서 오류가 더이상 줄어들지 않을 때까지 반복
   - 룰 포스트 가지치기 : 뿌리 노드부터 잎 노드까지 경로 형태로 변환한 뒤 정확도 낮은 순서대로 제거
6. 타당성 평가 : 이익 도표, 위험 도표, 교차 타당성(교차 검증)으로 평가
7. 해석 및 예측



**의사결정나무 대표적 알고리즘**

1. CART

   - 가장 일반적 
   - 불순도 측도 : 범주형이나 이산형 > 지니 지수 / 연속형 > 분산의 감소량을 이용한 이진분리

2. C4.5 / C5.0

   - 범주형/이산형 목표변수에만 활용
   - 불순도 측도 : 엔트로피 지수 활용 
   - 각 마디에서 다지분리 가능

3. CHAID

   - 불순도 측도 : 카이제곱 통계량 활용
   - 가지치기 안하고 적당한 크기에서 성장 중지
   - 다지분리 가능

   | CHAID     | 카이제곱 통계량 | ANOVA F-통계량 |
   | --------- | --------------- | -------------- |
   | CART      | 지니 지수       | 분산 감소량    |
   | C4.5/C5.0 | 엔트로피 지수   |                |

4. 랜덤 포레스트

   - 부트스트래핑 기반 샘플링 활용한 의사결정나무 생성 후 배깅 기반 나무들 모아서 앙상블 학습 > 숲 형성
   - 부트스트래핑 : 단순 복원 임의추출법으로 크기 동일한 여러 개 표본자로 생성
   - 배깅 : 여러 부트스트랩 자료 생성해서 학습, 분류기 생성 후 그 결과 앙상블
     - 범주형 데이터면 다수결 투표 방식
     - 연속형 데이터면 평균으로 결과 집계
   - 부스팅 : 가중치 활용해서 약분류기를 강분류기로 

   - 앙상블 학습 : 일반화 성능 향상시켜서 과적합 해결 가능



**의사결정나무 장단점**

- 연속형, 범주형 변수 모두 적용, 변수 비교 가능
- 규칙 도출하는 데 유용
- 트리구조가 복잡하면 예측/해석력 떨어짐
- 데이터 변형에 민감



**인공신경망(ANN)**

> 다른 뉴런들과 연결돼서 신호를 전달하고 처리하는 구조
>
> 신호의 강도에 따라 가중치가 처리되고 활성화 함수를 통해서 출ㄺ이 계산
>
> 학습을 거쳐서 원하는 결과가 나오게끔 가중치가 조정된다는 점이 특징
>
> 높은 복잡성으로 입력 자료의 선택에 민감

- 범주형 변수 : 범주가 일정한 구간이어야
- 연속형 변수 : 범위가 큰 차이가 없어서 표준화가 가능한 경우에 적합



**인공신경망의 발전**

1. 기존 신경망 다층 퍼셉트론이 가진 문제
   - 사라지는 경사도 : 신경망 층수 늘릴 때 데이터가 사라져서 학습이 잘 안되는 문제
   - 과대적합
2. 딥러닝 등장
   - 사전학습으로 사라지는 경사도 문제 해결
   - 초기화 알고리즘의 발전과 드롭아웃을 사용해서 과대적합 방지
   - 기본 구조인 DNN은 은닉층을 2개 이상 가진 학습 구조(CNN, RNN, LSTM, GRU, 오토인코더, GAN 등)



**인공신경망 원리**

> 적절한 출력값을 생성하기 위해 가중치 W를 곱한 값에 편향을 더함
>
> 이를 조정하면서 학습하고 최적화 > 최종적으로 활성화 함수 활용

- 주요 요소
  - 노드, 가중치, 활성함수, 입력층, 은닉층, 출력층
- 뉴런 간 연결 방법
  - 층간 연결 : 서로 다른 층에 존재하는 뉴런과 연결
  - 층내 연결 : 동일 층 내의 뉴런과의 연결
  - 순환 연결 : 어떠한 뉴런의 출력이 자기 자신에게 입력되는 연결



**학습**

> 적응 가능한 가중치와 편향이 있고 이를 훈련 데이터에 적응하도록 조정하는 과정 > 학습

- 손실 함수
  - 신경망이 출력한 값과 실제값과의 오차에 대한 함수
  - 손실함수값이 최소화되도록 가중치와 편향을 찾는 것을 학습
  - 일반적으로 평균제곱 오차 OR 교차엔트로피 오차를 활용
    - 평균제곱 오차(MSE) : 출력값과 사용자가 원하는 출력값 사이의 거리 차이를 오차로 사용 > 각 거리 차이 제곱해서 합산 후 평균
    - 교차 엔트로피 오차(CEE) : 분류 부문으로, 출력값에 자연로그를 적용하고 곱
- 학습 알고리즘
  1. 미니배치 : 훈련 데이터 중 일부를 무작위로 선택한 데이터(손실함수 줄이는 게 목표)
  2. 기울기 산출 : 경사법으로 가중치 매개변수의 기울기를 일반적으로 미분을 통해서 구함
     - 경사 하강법, 경사 상승법, 무작위 미니배치를 통한 확률적 경사 하강법
  3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 업데이트하면서 단계 반복
- 오차역전파
  - 기울기를 미분을 통해서 하면 시간 소모가 커서 
  - 오차를 출력층에서 입력층으로 전달, 연쇄법칙을 활용한 역전파를 통해서 계산하고 업데이트
  - 연쇄법칙 : 합성함수의 미분은 각각의 미분의 곱으로 가능