## 1. 분석 절차 수립



**분석 모형 선정 필요성**

1. 의사 결정
2. 불확실성 해소
3. 요약
4. 인과관계 파악
5. 예측



**분석 모형 선정 프로세스**

1. 문제요건 정의 또는 비즈니스 이해에 따른 대상 데이터 선정과 분석 목표/조건 정의 
2. 데이터 수집, 정리 및 도식화
3. 데이터 전처리
4. 최적의 분석 모형 선정



**분석 모형 종류**

1. 예측 분석 모형
   - 어떤 일들이 발생할 것인가?
2. 현황 진단 모형
   - 과거에 어떠한 상황이 왜 어떻게 일어났는가?
3. 최적화 분석 모형
   - 어떻게 하면 원하는 결과가 일어날 수 있을까?



**분석 모형 정의 위한 사전 고려사항**

- 상향식
- 하향식
  - 비즈니스 모델
    - 어떻게 수익을 창출할 것인가
  - 외부 참조 모델
    - 벤치마킹으로 분석 테마 후보 pool을 구축, 선택
  - 분석 유즈케이스 기반 모델
    - 문제에 대한 상세 설명과 해결 시 효과에 대해 명시



**분석 모형 설계**

> 분석 대상 및 범위 정해서 분석 목적 구현하기 위한 분석방법론 설계하는 단계

1. 사전 확인 사항
2. 분석 모델링 설계와 검정
   - 가설검정 방법 수립
   - 추정방법에 대한 기술 검토
   - 분석 모델링 설계와 검정 방법 수립
3. 적합한 알고리즘 설계
   - 비지도학습 : 군집분석, 연관성분석, 오토인코더 등
   - 지도학습 : 의사결정트리, 랜덤 포레스트, 서포트벡터머신, 회귀분석 등
   - 준지도학습 : 셀프 트레이닝, 적대적 생성 모델 등
   - 강화학습 : Q-Learning, 정책경사 등
4. 개발 및 테스트



**통계적 가설검정**

> 통계적 추측의 하나
>
> 모집단의 실제값이 얼마가 된다는 주장에 대해 표본의 정보를 사용해서 가설의 합당성 여부 판정하는 과정
>
> = 가설검정
>
> 통계적 가설 : 특정 주장을 모수를 이용해서 나타낸 형태를 지칭 > 미국 성인여자의 평균 신장은 180이다
>
> 귀무가설과 대립가설로 나타남



**가설검정 5단계**

1. 유의수준 결정, 귀무가설과 대립가설 설정
   - 귀무가설 : 직접 검정이 되는 가설(영가설)(기각되는 것이 예상되어 세워진 가설)
   - 귀무가설이 옳다는 가정 하에 시작
   - 진실일 가능성이 적어서 처음부터 기각될 것이 예상되는 가설
2. 검정통계량의 설정
   - 가설을 검정하기 위한 기준으로 사용하는 값
3. 기각역의 설정
   - 귀무가설 기각하는 영역
4. 검정통계량 계산
   - (표본평균 - 모평균) / 표본 표준편차
   - 신뢰수준 : 어느 정도로 검정할 것인지에 대한 수준
   - 유의수준 : 유의수준 벗어나면 귀무가설 오류로 판단 (1 - 신뢰수준) > 알파값(α)
5. 통계적인 의사결정(가설검정)
   - 양측검정
   - 단측검정
   - 검정통계량을 t값 분포도와 비교해서 기각역에 속하는지 아닌지 판단



**분석모델링 설계와 검정**

- 분석모형
  - 예측 분석
  - 현황 진단
  - 예측 최적화
  - 종속변수 유무에 따라 사용할 알고리즘이 제한됨
    - 종속변수 없으면 군집과 원인분석, 이상치, 연관 법칙 등으로 제한
  - 변수의 속성에 따라 알고리즘 선택 달라짐



---

## 2. 분석 환경 구축



**R**

- 객체지향 언어
- 고속메모리 처리
- 대용량 메모리 처리 어려움
- 보안 기능 취약
- 별도 모듈 연동 아니면 웹 브라우저에서 사용 못 함



**파이썬**

- 인터프리터 언어
- 동적인 데이터타입 결정 지원
- 플랫폼 독립적 언어



**과대적합**

- 방지를 위해 데이터 분할과 k-fold 교차검증, 정규화 등의 방법이 있음



---



## 3. 분석기법



**학습 유형에 따른 데이터 분석 모델**

1. 지도학습 : 대표적으로 분류와 회귀로 구분
   - 분류 : 의사결정트리(분류), 랜덤 포레스트, 인공신경망(지도학습), 서포트벡터머신, 로지스틱 회귀분석
   - 회귀 : 의사결정트리(회귀), 선형회귀분석, 다중회귀분석
2. 비지도학습 
   - 예를 들어 전기자동차를 산 구매자들의 데이터를 가지고 있을 때, 어떤 사람들이 주로 전기자동차를 구매하는지 알아봄
   - 입력값은 있으나 정답이 없어서 출력값이 존재하지 않으므로 학습모델의 성능을 평가하기 어려움
   - 군집분석, 연관성분석, 인공신경망, 오토인코더
3. 준지도학습
   - 정답있는 데이터와 없는 데이터 동시에 학습
   - 데이터 라벨링을 하는 데 훈련된 사람의 손을 거쳐야 하므로 비용 크게 듦
   - 레이블 된 소수의 데이터만으로 부분학습모델을 만들고, 나머지 레이블이 없는 데이터에 레이블 생성 후 지도학습
   - 셀프 트레이닝, GAN
     - 셀프 트레이닝 : 정답있는 데이터로 학습한 뒤 정답없는 데이터로 예측해서 가장 높은 확률값 데이터만 다시 정답 데이터로 가져가고 > 이걸 반속해서 높은 확률값이 나오는 데에 가중치를 둠
     - GAN(생성적 적대 신경망) : 위조지폐범과 경찰 사이의 게임과 같음(생성모델과 판별모델로 나뉨)
4. 강화학습
   - 주어진 환경에서 보상 최대화하도록 에이전트 학습

| 비지도학습 | 군집분석, 연관성 분석, 인공신경망, 오토인코더 |
| ---------- | --------------------------------------------- |
| 준지도학습 | 셀프 트레이닝, GAN                            |
| 강화학습   | Q-Leaning, 정책경사(PG)                       |



**데이터 분석 알고리즘과 분야**

| 업리프트 모델링             | 단계적 추정, 예측 분석          |
| --------------------------- | ------------------------------- |
| 회귀분석                    | 예측, 추정                      |
| 시각화                      | 원인과 관계                     |
| 부스팅, 배깅                | 분류 분석                       |
| 시계열분석                  | 시간상의 예측(이자율)           |
| 요인분석                    | 차원축소                        |
| 텍스트 마이닝               | 감성 분석                       |
| 의사결정 나무, 랜덤포레스트 | 분류                            |
| 신경 회로망                 | 예측 분석                       |
| 군집분석                    | 독립변수들만의 분류, 그룹화     |
| 앙상블 기법                 | 추정, 예측, 규범 등의 결합 분석 |
| 서포트벡터머신              | 분류 분석                       |
| 주성분분석                  | 원인 분석, 차원축소             |

- 업리프팅 모델 > 마케팅 캠페인에서 많이 사용(추정 모델을 단계별로 적용)
- 로지스틱 회귀분석은 이진분류에 자주 활용
- 시계열 분석 > 이자율이나 주식 예측 등에 자주 사용(이상치 감지 기법)



**회귀분석**

- 독립변수 : 입력값, 원인을 설명하는 변수
- 종속변수 : 결과값, 효과를 설명하는 변수
- 회귀선(회귀계수) : 독립변수 주어질 때 종속변수의 기대값
- 최소제곱법 : 잔차 제곱의 합이 최소가 되게 하는 직선을 찾는 방법



**회귀분석 모형 진단**

1. 적합도 검정
   - 회귀식이 표본의 실제값을 얼마나 잘 설명하는지
   - R2(결정계수) > SSR/SST 
     - SSR : 회귀모형에 의해 설명되는 변동의 제곱합
     - SSE : 오차에 의해 설명되는 변동의 제곱합
     - SSR + SSE = SST
2. 변수 영향력 분석
   - 종속변수에 독립변수들이 얼만큼 영향력을 미치는 지 
   - p값이 0.05보다 작으면 통계적으로 유의미하다
3. 그 외(잔차분석 등)



**선형회귀분석**

> 종속변수와 독립변수 모두 연속형이어야

1. 단순 선형회귀분석
2. 다중 선형회귀분석

- 기본적 가정
  - 선형성
  - 잔차 정규성
  - 잔차 독립성
  - 잔차 등분산성
  - 다중 공선성



**로지스틱 회귀분석**

> 다른 것들은 선형회귀분석과 유사하지만
>
> 종속변수가 연속형이 아닌 범주형으로 주어지면, 특정 분류로 결과가 나타남

1. 단순 로지스틱 회귀분석 : 종속변수가 이항형 문제
2. 다중 로지스틱 회귀분석

- 승산으로 로짓변환을 통해 선형함수로 치환 가능
  - 승산 : P(A) / P(Ac)

- 복수의 범주이면서 순서가 존재하면 > 서수 로지스틱 회귀



**의사결정나무**

> 전체자료를 몇 개의 소집단으로 분류하거나 예측을 수행

- 종류
  - 분류나무 : 이산형(범주형) 목표변수에 따른 빈도기반 분리
    - 가지분할 진행 시 : 카이제곱 통계량의 p값, 지니지수, 엔트로피 지수 등이 분리 기준
    - 분리 기준을 선택할 때 불순도를 감소시키도록 설정해야
    - 불순도 차이 : 정보 획득
    - 카이제곱 : ((실제도수 - 기대도수)2 / 기대도수)의 합
    - 지니지수 : 특정 집합에서 한 항목을 뽑아서 무작위로 라벨 추정시 틀릴 확률 >> 1 - P(A) - P(Ac)
    - 엔트로피 지수 : 무질서 정도에 대한 측도
  - 회귀나무 : 연속형 목표변수에 따른 평균/표준편차 기반 
    - 가지분할 진행 시 : F-통계량의 p값, 분산의 감소량 등이 분리 기준
    - F 통계량의 p값 : 등분산성을 검정해서 p값이 커지면 등분산성 있음 = 낮은 이질성 = 순수도 높아짐
    - 분산의 감소량 : 최대화가 될수록 낮은 이질성 = 순수도 높아짐



**의사결정나무 분석 과정**

1. 변수 선택
2. 의사결정나무 형성 : 적절한 분리기준과 정지규칙, 평가기준 등으로 의사결정 나무 만들기
3. 가지치기 : 오버피팅 막고 일반화 성능 높임
4. 모형 평가 및 예측 : 이익, 위험, 비용 고려
   - 재귀적 분기 학습 : 분기 전보다 분기 후 정보 획득량 높아지도록 분기 반복(모든 잎 엔트로피가 0될 때까지 반복)
5. 가지치기 : 평가용 데이터 활용
   - 에러감소 가지치기 : 오류를 비교해서 오류가 더이상 줄어들지 않을 때까지 반복
   - 룰 포스트 가지치기 : 뿌리 노드부터 잎 노드까지 경로 형태로 변환한 뒤 정확도 낮은 순서대로 제거
6. 타당성 평가 : 이익 도표, 위험 도표, 교차 타당성(교차 검증)으로 평가
7. 해석 및 예측



**의사결정나무 대표적 알고리즘**

1. CART

   - 가장 일반적 
   - 불순도 측도 : 범주형이나 이산형 > 지니 지수 / 연속형 > 분산의 감소량을 이용한 이진분리

2. C4.5 / C5.0

   - 범주형/이산형 목표변수에만 활용
   - 불순도 측도 : 엔트로피 지수 활용 
   - 각 마디에서 다지분리 가능

3. CHAID

   - 불순도 측도 : 카이제곱 통계량 활용
   - 가지치기 안하고 적당한 크기에서 성장 중지
   - 다지분리 가능

   | CHAID     | 카이제곱 통계량 | ANOVA F-통계량 |
   | --------- | --------------- | -------------- |
   | CART      | 지니 지수       | 분산 감소량    |
   | C4.5/C5.0 | 엔트로피 지수   |                |

4. 랜덤 포레스트

   - 부트스트래핑 기반 샘플링 활용한 의사결정나무 생성 후 배깅 기반 나무들 모아서 앙상블 학습 > 숲 형성
   - 부트스트래핑 : 단순 복원 임의추출법으로 크기 동일한 여러 개 표본자로 생성
   - 배깅 : 여러 부트스트랩 자료 생성해서 학습, 분류기 생성 후 그 결과 앙상블
     - 범주형 데이터면 다수결 투표 방식
     - 연속형 데이터면 평균으로 결과 집계
   - 부스팅 : 가중치 활용해서 약분류기를 강분류기로 

   - 앙상블 학습 : 일반화 성능 향상시켜서 과적합 해결 가능



**의사결정나무 장단점**

- 연속형, 범주형 변수 모두 적용, 변수 비교 가능
- 규칙 도출하는 데 유용
- 트리구조가 복잡하면 예측/해석력 떨어짐
- 데이터 변형에 민감



**인공신경망(ANN)**

> 다른 뉴런들과 연결돼서 신호를 전달하고 처리하는 구조
>
> 신호의 강도에 따라 가중치가 처리되고 활성화 함수를 통해서 출ㄺ이 계산
>
> 학습을 거쳐서 원하는 결과가 나오게끔 가중치가 조정된다는 점이 특징
>
> 높은 복잡성으로 입력 자료의 선택에 민감

- 범주형 변수 : 범주가 일정한 구간이어야
- 연속형 변수 : 범위가 큰 차이가 없어서 표준화가 가능한 경우에 적합



**인공신경망의 발전**

1. 기존 신경망 다층 퍼셉트론이 가진 문제
   - 사라지는 경사도 : 신경망 층수 늘릴 때 데이터가 사라져서 학습이 잘 안되는 문제
   - 과대적합
2. 딥러닝 등장
   - 사전학습으로 사라지는 경사도 문제 해결
   - 초기화 알고리즘의 발전과 드롭아웃을 사용해서 과대적합 방지
   - 기본 구조인 DNN은 은닉층을 2개 이상 가진 학습 구조(CNN, RNN, LSTM, GRU, 오토인코더, GAN 등)



**인공신경망 원리**

> 적절한 출력값을 생성하기 위해 가중치 W를 곱한 값에 편향을 더함
>
> 이를 조정하면서 학습하고 최적화 > 최종적으로 활성화 함수 활용

- 주요 요소
  - 노드, 가중치, 활성함수, 입력층, 은닉층, 출력층
- 뉴런 간 연결 방법
  - 층간 연결 : 서로 다른 층에 존재하는 뉴런과 연결
  - 층내 연결 : 동일 층 내의 뉴런과의 연결
  - 순환 연결 : 어떠한 뉴런의 출력이 자기 자신에게 입력되는 연결



**학습**

> 적응 가능한 가중치와 편향이 있고 이를 훈련 데이터에 적응하도록 조정하는 과정 > 학습

- 손실 함수

  - 신경망이 출력한 값과 실제값과의 오차에 대한 함수
  - 손실함수값이 최소화되도록 가중치와 편향을 찾는 것을 학습
  - 일반적으로 평균제곱 오차 OR 교차엔트로피 오차를 활용
    - 평균제곱 오차(MSE) : 출력값과 사용자가 원하는 출력값 사이의 거리 차이를 오차로 사용 > 각 거리 차이 제곱해서 합산 후 평균
    - 교차 엔트로피 오차(CEE) : 분류 부문으로, 출력값에 자연로그를 적용하고 곱

- 학습 알고리즘

  1. 미니배치 : 훈련 데이터 중 일부를 무작위로 선택한 데이터(손실함수 줄이는 게 목표)
  2. 기울기 산출 : 경사법으로 가중치 매개변수의 기울기를 일반적으로 미분을 통해서 구함
     - 경사 하강법, 경사 상승법, 무작위 미니배치를 통한 확률적 경사 하강법
  3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 업데이트하면서 단계 반복

- 오차역전파

  - 기울기를 미분을 통해서 하면 시간 소모가 커서 
  - 오차를 출력층에서 입력층으로 전달, 연쇄법칙을 활용한 역전파를 통해서 계산하고 업데이트
  - 연쇄법칙 : 합성함수의 미분은 각각의 미분의 곱으로 가능
  - 역전파 처리 : 렐루계층, 시그모이드 계층, 아핀계층, 등

- 활성화 함수

  - 입력 신호 총합을 그대로 사용하지 않고 출력신호로 변환하는 함수
  - 대표적으로 시그모이드, 렐루
  - 퍼셉트론 : 1개 이상의 입력층과 1개 출력층 뉴런으로 구성된 활성화 함수에 따라 출력되는 신경망구조
  - 다층 퍼셉트론은 은닉층이 1개 이상의 퍼셉트론, 계단 함수 사용해서 0 또는 1을 반환
  - 딥러닝 인공신경망은 시그모이드를 포함한 다른 활성화함수들을 사용해서 자동으로 학습
  - Sigmoid(이진분류) : 0~1
  - Relu(이진분류) : 0보다 작으면 0, 나머지는 입력값 그대로 출력

- 과대적합

  - 해결방안 : 가중치 매개변수 절대값을 작게 만드는 가중치 감소, 일정 비율 뉴런만 학습하는 드롭아웃, 하이퍼파라미터 최적화 방법

  1. 가중치 감소
     - 가중치 클수록 패널티 부과(패널티 역할로 규제(정규화))
     - Lasso(라쏘) : L1 규제 : 벡터 요소의 모든 절대값을 합한 값(맨하탄 or 택시 norm)
     - Ridge(릿지) : L2 규제 : 비용함수를 조정, 벡터의 유클리드 거리값(최단거리)
     - L2규제가 많이 활용됨
  2. 드롭아웃
     - 은닉층의 뉴런을 임의로 삭제하면서 학습
  3. 초매개변수
     - 학습률, 배치크기, 훈련 반복 횟수, 가중치 초기화 방법 등 수동으로 설정하는 변수로 최적화된 하이퍼파라미터 도출



**딥러닝 모델 종류**

1. CNN(합성곱 신경망 모델)

   - 인접하는 계층의 모든 뉴런과 결합된 완전 연결을 구현한 아핀 계층 사용

   - 모든 입력 데이터들을 동등한 뉴런으로 처리

   - 특징 추출하는 과정 : 합성곱 계층, 풀링 계층으로 나뉨

   - 합성곱 계산 뒤 특징지도(피처맵) 생성

     - 특징지도는 서브샘플링을 통해서 차원을 줄여줌
     - 필터 크기, 스트라이드, 패딩 적용여부, 최대풀링 크기에 따라 출력 데이터의 구조가 결정

     1. 필터 : 이미지 특징 찾기 위한 정사각형 행렬 (커널)

     2. 스트라이드 : 필터는 입력 데이터를 일정한 간격인 스트라이드로 순회하면서 특징 추출 > 특징지도 만들어짐
     3. 패딩 : 입력데이터 주변을 특정값으로 채우는 것(입력데이터 크기보다 작은데 출력데이터 크기가 줄어드는 걸 방지하고자)

   - 합성곱 계층

     - 2차원 입력데이터가 들어오면 필터 윈도우를 일정 간격으로 이동하면서 연산
     - 필터의 매개변수가 완전연결 신경망의 가중치에 해당
     - 합성곱 신경망을 통해서 학습이 반복되고 원소값이 매번 갱신되며 편향은 동일하게 하나만 존재
     - 모든 채널의 필터 크기가 동일해야
     - 필요 요소 : 패딩(연산 전 입력데이터 주위를 0이나 1로 채워서 설정)과 스트라이드(간격이 넓어져서 출력 데이터 크기가 줄어듦)

   - 풀링 계층

     - 선택적 요소
     - 2차원 데이터의 세레와 가로 방향의 공간을 줄이는 연산
     - 최대 풀링, 평균 풀링

   - 평탄화 계층을 이용해서 데이터 배열 형태로 처리한 뒤 완전연결계층을 통해 최종 클래스가 분류됨

   - LeNeT, Alexnet, VCG, GoogleLeNet, ResNet ...

2. RNN

   - 순서를 가진 데이터를 입력해서 방향성 그래프를 형성
   - 내부 상태(메모리) 이용해서 입력 시퀀스 처리
   - 은닉층이 순환구조로 동일한 가중치를 공유(cnn과 다른 점)
   - 가중치와 편향에 대한 오차함수의 미분을 계산하기 위해 확률적 경사하강법(SGD) 이용
   - 과거시점까지 역전파하는 BPTT를 활용
   - 입력 시점마다 가중치 공유, 계산 기울기는 현재 상태와 이전 상태에 대해 의존적 > 순차적 데이터 처리에 유용

3. LSTM

   - RNN은 데이터가 점차 소멸하는 문제점을 가지고 있음(거리가 멀면 기울기가 점차 줄어들어서 학습 능력 떨어짐)
   - 단점 보완된 알고리즘(4배 이상 파라미터를 보유해서 오랜 시간 데이터를 잘 기억함)
   - 입력 게이트, 출력 게이트, 망각 게이트 세 가지 게이트로 활성화 함수 거치지 않고 컨트롤 게이트를 통해서 값을 조절
   - 셀이라는 층이 있음(장기 메모리를 기억하는 셀)

4. 오토인코더

   - 대표적 비지도학습 모델
   - 다차원을 저차원으로 바꾸고 또 저차원을 다시 고차원으로 바꾸면서 특징점 찾아냄
   - 차원을 줄이는 은닉층으로 보내고 디코더를 통해서 차원을 늘림
   - 디노이징 오토인코더 : 손상있는 입력값 받아도 손상 제거하고 원본 데이터를 출력값으로 만듦
   - 희소 오토인코더 : 매번 일부 노드만 학습해서 과적합 해결
   - VAE : 확률분포를 학습하면서 데이터 생성
   - 데이터 압축, 저차원화를 통한 데이터 관찰, 배경잡읍 억제 등

5. GAN 

   - 판별자 네트워크 : 랜덤 노이즈 M개 생성해서 2M개 데이터 만들고 정확도를 최대화하는 방향
   - 생성자 네트워크 : 랜덤 노이즈 M개 재생성해서 정확도를 최소화하도록
   - 균형있는 경쟁이 필요하지만, 한 쪽으로 치우져치면 DCGAN을 통해서 함



**인공신경망의 장단점**

- 비선형적 예측 가능
- 다양한 데이터 유형, 새로운 학습 환경, 불완전한 데이터 입력 등에도 적용 가능
- 설명기능이 떨어짐 > 하지만 대체안이 연구되고 있음
- 데이터 커질수록 학습시키는 데 시간과 비용이 커짐



---



## 4. 고급 분석기법



**범주형 자료분석**

> 일반적으로 그 빈도를 세서 표를 작성함
>
> 만약 두 변수의 범주가 교차되어 있으면 분할표



**자료의 형태에 따른 범주형 자료 분석 방법**

| 독립변수 | 종속변수 | 분석방법                              | 예제                      |
| -------- | -------- | ------------------------------------- | ------------------------- |
| 범주형   | 범주형   | 빈도분석, 카이제곱 검정, 로그선형모형 | 지역별 선호정당           |
| 연속형   | 범주형   | 로지스틱 회귀분석                     | 소득에 따른 결혼의 선호도 |
| 범주형   | 연속형   | T검정(2그룹), 분산분석(2그룹 이상)    | 지역별 가계수입의 차이    |
| 연속형   | 연속형   | 상관분석, 회귀분석                    |                           |



**분할표**

> 각 변수에 따라서 통계표 형태로 정리

- 차원 : 분할표의 구성에 관계된 변수의 수
- 수준 : 범주형 변수가 가지는 범주의 수
- 비율의 차이(D) 
  - -1 ~ 1 사이 
  - 동질, 독립인 경우 D = 0
- 상대적 위험도(RR)
  - 0 ~ ∞ 사이
  - 동질, 독립일 때 RR = 1
- 오즈비(OR)
  - 0 ~ ∞ 사이
  - 동질, 독립일 때 OR = 1



**빈도분석**

> 질적자료를 대상으로 빈도와 비율을 계산할 때 사용
>
> 질적자료와 양적자료가 많을 때 질적자료를 대상으로 오류가 있는지 확인



**교차분석(카이제곱검정)**

> 두 범주형 변수가 서로 상관이 있는지 독립인지 판단



**로지스틱 회귀분석**

> 분석 대상들이 두 집단 OR 그 이상의 집단으로 나뉘어진 경우 개별 관측치들이 어느 집단으로 분류될 수 있는지 분석



**T검정**

> 독립변수가 범주형이고 종속변수가 연속형일 때
>
> 두 집단 간 평균 비교 등



**분산분석**

> 두 집단 간의 분산 비교 등에 사용



**다변량분석**

- 조사 중인 각 개인 OR 대상물에 댛나 다수의 측정치를 동시에 분석하는 모든 통계적 방법
- 종속변수의 관계성을 고려해서 여러 개의 단변량분석을 동시에 수행



**다변량분석 용어**

1. 종속 기법 : 종속변수와 독립변수로 구분해서 독립변수들이 종속변수에 미치는 영향력 분석
2. 상호의존적 기법 : 구분하지 않고 그냥 전체를 대상으로 분석
3. 명목 척도 : 숫자 그 자체에는 전혀 의미없는 측정단위
4. 순위 척도 : 선호되는 순위 나타낸 숫자, 숫자 자체는 의미있지만 나머지는 의미없음
5. 등간 척도 : 숫자 자체와 숫차의 차이는 의미를 가짐, 하지만 비율은 의미 없음
6. 비율 척도 : 모든 의미를 다 가지는 가장 높은 측정단위
7. 정량적 자료 : 등간척도, 비율척도로 측정된 자료 / 양적자료 or 모수화된 자료
8. 비정량적 자료 : 명목척도나 순위척도로 측정된 자료 / 질적자료 or 비모수화된 자료
9. 변량 : 변수들을 통계적 방법으로 가중치를 줘서 변수들의 합의 형태로 나타낸 새로운 변수



**다변량분석기법의 분류**

1. 다중회귀분석
   - 하나의 종속변수와 하나 이상의 독립변수 간 관련성 있다고 가정되는 문제에 적합
   - 다수의 독립변수의 변화에 따른 종속변수의 변화 예측
2. 다변량분산분석, 다변량공분산분석
   - 다변량 분산분석(ANOVA)
     - 두 개 이상의 독립변수와 다수의 종속변수 간 관련성 동시에 알아볼 때
   - 다변량공분산분석(ANCOVA)
     - 통제되지 않은 독립변수들의 종속변수들에 대한 효과 제거하기 위해 이용
3. 정준상관분석
   - 하나의 종속변수와 다수의 독립변수 간 관련성 조사하는 다중회귀분석을 논리적으로 확대
   - 상관을 가장 크게 하는 각 변수군의 선형조합 찾아냄(각 변수의 가중치의 집합을 찾아내기)
4. 요인분석
   - 많은 수의 원래 변수들을 적은 수의 요인으로 요약하기 위함
   - 측정도구의 타당성을 파악할 때 많이 사용
5. 군집분석
   1. 표본들 간의 유사성, 연관성 조사
   2. 표본을 분류해서 넣거나 소속 예측
   3. 군집기법에 의해 나타난 그룹들에 대해 판별분석 적용
6. 다중판별분석
   - 종속변수가 비계량적 변수일 때
   - 집단 간의 차이를 판별함
   - 특정 집단에 속할 가능성 예측
7. 다차원척도법(MDS)
   - 거리 또는 비유사성을 이용해서 원래 차원보다 낮은 차원으로 위치시켜서 쉽게 파악
   - 주관적 해석에 중점



**시계열자료**

1. 이산시계열 : 이산적 형태로 분리되어 존재
2. 연속시계열 : 연속적으로 연결된 형태
3. 시차



**시계열자료의 성분**

1. 불규칙 성분
2. 체계적 성분
   - 추세성분
   - 계절성분
   - 순환성분 : 주기가 긴 변동을 가지는 형태
   - 복합성분 : 추세성분과 계절성분 동시에 가짐
   - 자기상관성 : 시차값들 사이에 선형관계
   - 백색잡음



**정상성**

> 시계열 데이터가 평균과 분산이 일정한 경우

1. 평균이 일정
   - 평균이 일정하지 않으면 차분을 통해서 정상성 가짐
2. 분산이 일정
   - 분산이 일정하지 않으면 변환을 통해서 정상성 가짐
3. 공분산의 경우도 단지 시차에만 의존해서 특정시점에는 의존하지 않음
4. 정상성 가지는 시계열 자료의 특징
   - 특정기간에서 얻은 정보를 다른 시기에서도 사용이 가능한 정보로 일반화 가능



**시계열자료 분석 방법**

- 예측목적
  - 단순 방법
    - 추세분석 이동평균
    - 평활법
    - 분해법
  - 모형 기반
    - 자기회귀모형(AR)
    - 자기회귀이동평균모형(ARMA)
    - 자기회귀누적이동평균모형(ARIMA)
- 이해와 제어의 목적
  - 스펙트럼 분석
  - 개입분석



**단순방법**

- 이동평균법
  - 일정기간을 시계열을 이동하면서 평균 계산
  - 추세 파악해서 시계열의 다음기간을 예측하는 데 사용
  - 뚜렷한 추세가 있거나 불규칙한 움직이 적으면 N을 작게
- 지수평활법
  - 관찰기간의 제한이 없이 모든 시계열 데이터 사용 > 최근 시계열에 더 많은 가중치
  - 단기간에 발생하는 불규칙 변동을 평활하는데 주로 사용
  - 과거 데이터일수록 가중치 적게
  - 중기 이상의 예측에 주로 사용(단순지수평활법의 경우에는 장기추세나 계절성이 포함된 시계열 데이터에는 부적합)
  - λ가 작으면 지엽적 변화에 민감
- 분해법
  - 성분 분류대로 시계열 데이터 분해
  - 시계열이 체계적 성분과 불규칙적 성분으로 이루어져 있다고 가정
  - 시계열의 장기적 추이 분석하고 불규칙성분으로부터 불규칙이 발생한 시점 찾음
  - 계절조정 자료 제공



**모형에 의한 방법**

- 자기회귀모형(AR)
  - 과거의 패턴이 지속된다면 예측할 수 ㅇ
  - 어느 정도의 멀리 있는 과거 관측치까지 이용할 것인가에 대한 판단 중요
- 자기회귀이동평균모형(ARMA)
  - MA 모형을 유한 개의 백색잡음의 집합으로 결합된 형태의 데이터
- 자기회귀누적이동평균모형(ARIMA)
  - 비정상성을 가지는 시계열 데이터 분석에 많이 사용
  - D는 차분



**베이즈 추론**

> 추론대상의 사전확률과 추가적인 정보를 통해서 사후 확률을 추론

- 확률론적 의미해석(조건부확률)
  - 사전확률 P(A)
  - P(B|A)는 A가 발생했을 때 B가 발생할 조건부 확률
  - 사후 확률 P(A|B)는 B가 발생 시 조건하에서 A가 발생하는 확률



**베이즈 기법**

- 객관적 관점
  - 이성적, 보편적으로 증명되며 논리의 확장으로 설명 ㅇ
- 주관적 관점
  - 지식의 상태는 개인적인 믿음의 정도로 측정할 수 있다



**베이즈 기법 적용**

- 회귀분석모델

  - 선형회귀분석모델
    - 추정치와 실제의 차이를 최소화하는 것이 회귀분석모델의 목표
  - 기존 머신러닝 방법
    - 경사하강법과 같은 알고리즘을 통해 점진적으로 학습해서 매개변수 찾아감
  - 베이지안 확률론의 적용 개념
    - 사전확률을 안 상태에서 새로운 데이터가 관측되면 다음 번 학습의 사전확률로 사용하면서 점진적으로 매개변수들의 분포를 찾아나감

- 분류

  - 나이브 베이즈 분류

    - 특성들 사이의 독립을 가정하는 베이즈 정리를 적용한 확률 분류기

    - 나이브(순진한)

    - 일반적인 원칙에 근거한 여러 알고리즘들을 이용해서 훈련

    - 지도학습 환경에서 매우 효율적

    - 트레이닝 데이터 양 매우 적음

    - 조건부 확률 모델

    - 이벤트 모델 : 특성의 분포에 대한 여러 가정들

      - 종류

      - | 가우시안 나이브 베이즈 | 연속적인 값 벡터들이 가우시안 분포 딸느다고 가정 |
        | ---------------------- | ------------------------------------------------ |
        | 다항분포 나이브 베이즈 | 다항분포에 의해 생성된 이벤트의 경우 사용        |
        | 베르누이 나이브 베이즈 | 독립적인 이진 변수로 표현될 경우 사용(성공/실패) |



**딥러닝 분석의 개념**

1. 인공신경망(ANN)
   - 문제점 
     - 계산속도 저하
     - 초기치의 의존성
     - 과적합 문제
2. 딥러닝
   - 여러 비선형 변환기법의 조합을 통해서 높은 수준의 추상화를 시도
   - 인공신경망의 단점 극복
   - 은닉층 잧를 여러 개로 만들어서 여러 단계 거치도록 신경망 구성 > 정확도 엄청 향상



**딥러닝 분석 알고리즘**

1. 심층 신경망(DNN)
   - 여러 개의 은닉층으로 이뤄진 인공신경망
2. 합성곱 신경망(CNN)
   - 최소한의 전처리를 사용하도록 설계 < 다계층 퍼셉트론
   - 가중치와 통합 계층을 추가로 활용
   - 2차원 구조의 입려데이터 충분히 활용 ㅇ
   - 오차 역전파를 통해 훈련 ㅇ > 쉽게훈련되고 적은 수의 매개변수를 사용
   - 파생 알고리즘
     - 합성곱 심층 신뢰 신경망(CDBN) : CNN + DBN
3. 순환 신경망(RNN)
   - 유닛 사이의 연결이 순환적 구조를 가진 신경망
   - 내부 메모리 활용 가능(순방향 신경망과 달리) > 높은 인식률
   - 시퀀스 데이터를 모델링하기 위해 등장
   - 기억을 갖고 있다는 게 특징
   - 새로운 입력 들어올 때마다 기억 수정 > 결국 요약 정보로
   - 파생 알고리즘
     - LSTM, 완전 순환망 ...
4. 심층 신뢰 신경망(DBN)
   - 그래프 생성 모형
   - 계층 간 연결 있지만, 계층 내의 유닛 간에는 연결 없음
   - 선행학습을 통해 초기 가중치를 학습 > 역전파나 다른 알고리즘으로 가중치 미조정
   - 훈련용 데이터 적을 때 매우 유용



**비정형 데이터 분석**

> 텍스트나 파일을 파싱해서 메타구조를 갖는 데이터셋 형태로 바꾸고 정형 데이터 형태의 구조로 만들 수 있도록 아키텍처 구조 수정해야

- 데이터 마이닝
  - KDD라고 함
  - 분류, 군집화, 연관성, 연속성, 예측
  - 상식적 범위에서 부분적 데이터 다룬다는 한계
- 텍스트 마이닝
  - 자연어 처리 : 수학 통계적 도구 많이 활용, 기계학습 도구 활용
- 웹 마이닝
  - 데이터 마이닝 기술에 추가적 분석기법 필요
- 오피니언 마이닝
  - 주관적 정보 추출
  - 감정 분석
- 리얼리티 마이닝
  - 기계나 모션센서 등의 행동에서 비정형 데이터 추출



**앙상블 분석**

1. 약학습기 : 성공확률이 높은, 오차율이 일정 이하인 학습 규칙
2. 강학습기 : 약학습기로부터 만들어내는 강력한 학습 규칙



**앙상블 분석 종류**

1. 보팅 : 투표를 통해 결정하는 방식
   - 서로 다른 여러 학습 모델을 조합해서 사용(배깅과 다른 점)
   - 하드 보팅 : 결과물에 대한 최종값을 투표해서 결정
   - 소프트 보팅 : 확률값을 다 더해서 최종결뫄물에 대한 각각의 확률을 구한 뒤 값 도출
2. 부스팅 : 가중치를 활용해서 연속적인 약학습기를 생성하고 강학습기 만들기
   - 순차적 학습을 통해 가중치 부여 > 오차 보완
   - 병렬 처리에 어려움, 학습 시간 오래 걸림
3. 배깅 : 샘플 여러 번 뽑아서 각 모델을 학습시켜 결과물 집계
   - 데이터로부터 부트스트랩 > 모델 학습
   - 범주형 자료는 투표 방식, 연속형 자료는 평균 집계
   - 랜덤 포레스트



**비모수 통계**

