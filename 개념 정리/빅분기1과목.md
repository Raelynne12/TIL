## 1) 빅데이터 개요 및 활용



**데이터**

> 추론과 추정의 근거를 이루는 사실
>
> 단순한 객체로도 가치 있고 다른 객체와의 상호관계 속에서 더 큰 가치를 가짐



**데이터 구분**

- 정량적 데이터(quantitative data)
  - 주로 숫자
  - 정형, 반정형 데이터
- 정성적 데이터(qualittive data)
  - 주로 문자
  - 비정형 데이터



**데이터 유형**

- 정형
  - 연산 가능
  - 정해진 형식과 구조에 맞게 저장되도록 구성
- 반정형
  - 연산 불가능
  - 비교적 유연, 파일 형식 데이터
  - json, xml, rdf, html
- 비정형
  - 구조가 정해지지 않은 데이터



**데이터 근원에 따른 분류**

> 데이터의 수집과정은 데이터의 재생산 과정
>
> 재생산된 데이터는 가역과 불가역으로 나뉨

- 가역 데이터
  - 1:1관계
  - 원본
  - 환원이 가능한 데이터
  - 추적 가능
  - 변경사항 반영
  - 데이터 마트, 데이터 웨어하우스에 활용
- 불가역 데이터
  - 환원 불가능한 데이터
  - 변경사항 반영 안됨
  - 추적 불가능
  - 데이터 전처리, 프로파일 구성에 활용



**데이터의 기능**

> 암묵지와 형식지의 상호작용이 중요
>
> 암묵지 : 외부에 표출되지 않은 무형의 지식
>
> 형식지 : 형상화된 유형의 지식

1. 공통화
   - 서로의 경험이나 인식 공유하며 한 차원 높은 암묵지로 발전
2. 표출화
   - 암묵지가 구체화 > 형식지로
3. 연결화
   - 형식지를 재분류해서 체계화
4. 내면화
   - 전달받은 형식지를 다시 개인의 것으로



**DIKW 피라미드**

데이터 > 정보 > 지식 > 지혜



**데이터베이스 관리 시스템(DBMS)**

> SQL
>
> 데이터베이스에 접근할 때 사용하는 언어
>
> 테이블 단위로 연산 수행

- 관계형
  - 테이블로 표현
- 객체지향
  - 객체 형태로 표현
- 네트워크
  - 그래프 구조를 기반
- 계층형
  - 트리 구조를 기반



**데이터베이스 특징**

1. 통합된 데이터
   - 중복되어 저장되지 않음
2. 저장된 데이터
   - 접근할 수 있는 저장매체에 데이터 저장
3. 공용 데이터
   - 여러 사용자가 서로 다른 목적으로 이용
4. 변화되는 데이터
   - 현시점의 상태를 나타내며 지속적으로 갱신



**데이터베이스의 활용**

- OLTP(online transaction processing)
  - 데이터 갱신 위주
  - 현재 시점의 데이터만을 관리
  - 데이터 수시로 갱신
- OLAP(online analytical processing)
  - 데이터 조회 위주
  - 쉽고 빠르게 다차원적 데이터에 접근



**데이터 웨어하우스**

> 사용자 의사결정에 도움주기 위해 기관시스템의 데이터베이스에 축적된 데이터를 공통의 형식으로 변환해서 관리

- 특징
  - 주제지향성
  - 통합성
  - 시계열성
  - 비휘발성
- 구성
  - 데이터 모델
  - ETL(extract, transform, load)
  - ODS(operational data store)
  - DW 메타데이터
  - OLAP
  - 데이터마이닝
  - 분석 도구
  - 경영기반 솔루션



**빅데이터 등장과 변화**

- 사전처리에서 사후처리
- 표본조사에서 전수조사
- 질에서 양
- 인과관계에서 상관관계



**빅데이터 특징**

- 5V
  - volume(규모)
  - variety(유형)
  - velocity(속도)
  - veracity(품질)
  - value(가치)



**빅데이터 활용**

- 자원
  - 빅데이터
- 기술
  - 빅데이터플랫폼
  - AI
- 인력
  - 알고리즈미스트
  - 데이터사이언티스트



**데이터 산업의 진화**

1. 처리 시대
2. 통합 시대
   - 데이터 모델링과 데이터베이스 관리 시스템 등장
3. 분석 시대
   - 빅데이터 기술 등장
   - 데이터 리터러시(data literacy)
4. 연결 시대
   - 오픈 API 경제
5. 권리 시대
   - 개인이 자신의 데이터를 자신을 위해 사용
   - 마이데이터 등장



**데이터 산업의 구조**

- 인프라 영역
  - 데이터 수집, 저장, 분석, 관리 등
  - 하드웨어, 소프트웨어 영역
- 서비스 영역
  - 교육이나 컨설팅, 솔류션 제공
  - 데이터 그 자체 제공 or 가공한 정보 제공



---



## 2) 빅데이터 기술 및 제도



**빅데이터 플랫폼**

> 빅데이터 수집과 저장, 처리, 분석 등 전 과정을 통합적으로 제공



**빅데이터 플랫폼의 구조**

- 소프트웨어 계층
- 플랫폼 계층
- 인프라스트럭처 계층



**빅데이터 처리과정**

1. 생성
2. 수집
   - 로그 수집기나 센서 네트워크, open api 활용
   - 수집 및 변환 과정 모두 포함
3. 저장(공유)
   - 하둡, NoSQL 등 
   - 시스템 간 데이터 서로 공유 가능
4. 처리
   - 데이터 효과적으로 처리하는 기술 필요
   - 분산 병렬 및 인 메모리(In-memory) 방식으로 실시간 처리
   - 대표적으로 하둡의 맵리듀스
5. 분석
   - 통계분석, 데이터 마이닝, 텍스트 마이닝, 기계학습 등
6. 시각화



빅데이터 수집

- 크롤링(crawling)
- 로그 수집기
- 센서 네트워크(sensor network)
- RSS reader/open api
- ETL 프로세스
  - extract(추출), transform(변환), load(적재)



빅데이터 저장

- NoSQL
  - SQL을 사용하지 않는 DBMS와 데이터 저장장치
  - 원자성, 일관성, 독립성, 지속성 포기
  - 데이터 업데이트 즉각적으로 가능
  - cloudata, hbase, cassandra, mongoDB가 대표적
- 공유 데이터 시스템
  - 일관성, 가용성, 분할 내성 중에서 최대 두 개의 속성만 보유 가능(CAP 이론)
  - RDBMS보다 높은 성능과 확장성 제공
- 병렬 데이터베이스 관리 시스템
  - 여러 디스크에 질의, 갱신, 입출력 등 동시에 처리
- 분산 파일 시스템
  - 네트워크로 공유하는 여러 호스트의 파일에 접근할 수 있는 파일 시스템
  - GFS, HDFS, 아마존 S3 파일 시스템 대표적
- 네트워크 저장 시스템
  - SAN, NAS가 대표적



빅데이터 처리

- 분산 시스템과 병렬 시스템
  - 분산 : 단일 시스템인 것처럼 구동
  - 병렬 : 분할된 작업을 동시에 처리하여 계산 속도가 빠름
  - **분산 병렬 시스템**
- 분산 병렬 컴퓨팅
  - 다수의 독립된 컴퓨팅 자원을 네트워크상에 연결 > 제어하는 **미들웨어**를 이용해서 동작하게
  - 하둡
    - 분산 처리 환경에서 대용량 데이터 처리 및 분석 지원 오픈 소스 소프트웨어 프레임워크
    - HDFS(분산파일 시스템), Hbase(데이터베이스), 맵리듀스(분산 컴퓨팅 지원 프레임워크)로 구성
    - 수 천대의 장비에 대용량 파일을 나눠서 저장 가능
    - 실시간으로 처리 및 분석 가능
    - 최근에는 부족한 기능 보완한 하둡 에코시스템 등장
  - 아파치 스파크
    - 실시간 분산형 컴퓨팅 플랫폼
    - In-Memory 방식으로 처리 > 하둡보다 처리 속도 빠름
  - 맵리듀스
    - 방대한 양의 데이터를 신속하게 처리하는 프로그래밍 모델
    - 병렬 및 분산 처리 효과적으로
    - 런타임에서 작업이 맵리듀스 처리 성능에 많은 영향 미침
    - 분할 > 통합 및 재분할 > 셔플 > 리듀스작업 > 데이터 생성 및 처리 종료



빅데이터 분석

- 탐구 요인 분석(EFA)
  - 데이터 간 상호 관계 파악하여 분석
- 확인 요인 분석(CFA)
  - 통계적 기법 통해 데이터 분석
- 분석 방법
  - 분류(Classification)
  - 군집화(Clustering)
  - 기계학습(Machine Learning)
  - 텍스트 마이닝
  - 웹 마이닝
  - 오피니언 마이닝
  - 리얼리티 마이닝
  - 소셜 네트워크 분석
  - 감성 분석



**인공지능(AI)**

> 기계를 지능화하는 노력
>
> 인공지능 : 사람이 생각하고 판단하는 사고 구조를 구축하려는 전반적인 노력
>
> 기계학습 : 인공지능의 연구 분야 중 하나, 학습 능력같은 기능을 축적된 데이터를 활용해서 실현하고자 하는 기술
>
> 딥러닝 : 기계학습 중 하나, 많은 데이터를 이용해서 사람처럼 스스로 학습할 수 있도록 인공신경망 등의 기술 이용



- 딥러닝 특징
  - 깊은 구조에 의해 엄청난 양의 데이터 학습 가능



- 기계학습 종류
  - 지도학습
    - 분류모형
    - 회귀모형
  - 비지도학습
    - 군집분석
    - 오토인코더
    - 생성적 적대 신경망
  - 준지도학습
  - 강화학습



**인공지능 데이터 학습의 진화**

1. 전이학습(transfer learning)
   - 딥러닝 모형을 다른 문제를 해결하기 위해 사용할 때 적은 양의 데이터로도 좋은 결과 얻을 수 있음
2. 전이학습 기반 사전학습모형(pre-trained model)
   - 인지능력 갖춘 딥러닝 모형에 추가적인 데이터 학습시키기
   - 응용력 갖추는 것 또한 필수적
3. BERT(bidirectional encoder representations from transformers)
   - 구글에서 발표한 언어인식 사전학습모형
   - 신속한 학습 가능



**빅데이터와 인공지능의 관계**

1. 학습 데이터 확보
   - 양질의 데이터 확보는 결국 성공적인 인공지능 구현과 직결
   - 무한한 모수 추정이 필요 > 많은 양의 데이터 필요
2. 학습 데이터의 애노테이션 작업
   - 애노테이션을 통해 학습이 가능한 데이터로 가공하는 작업 필요
   - 수작업이 동반
3. 애노테이션 작업을 위한 도구로써의 인공지능
   - 전문으로 하는 기업 수 증가



**인공지능의 기술동향**

1. 기계학습 프레임워크 보급 확대
   - 텐서플로우 : 파이썬 기반 딥러닝 라이브러리
   - 케라스 : 딥러닝 신경망 구축 위한 단순화된 인터페이스를 가진 라이브러리
2. 생성적 적대 신경망(GAN)
   - 두 개의 인공신경망으로 구성된 딥러닝 이미지 생성 알고리즘
   - 가짜 사례 생성하면 감별자가 진위 판별하도록 구성 > 공방전 번복
   - 가짜 사례 정밀도를 높이는 방식으로 작동
3. 오토인코더
   - 라벨이 설정되어 있지 않은 학습 데이터로부터 더욱 효율적인 코드로 표현하도록 학습
   - 입력 데이터 차원 줄여 > 모형 단순화시키기
4. 설명 가능한 인공지능(XAI)
   - 결론 도출 과정에 대한 근거를 차트나 수치, 자연어 형태의 설명으로 제공
5. 기계학습 자동화
   - 기계학습 전체 과정을 자동화



**2020년 데이터 3법 주요 개정 내용**

- 데이터 이용 활성화 위한 가명정보 개념 도입 및 데이터 간 결합 근거 마련
- 거버넌스 체계 효율화
- 다소 모호했던 개인정보의 판단기준 명확화



**개인정보 비식별화 단계**

1. 사전 검토
2. 비식별 조치
3. 적정성 평가
4. 사후 관리



**데이터 위기 요인과 통제 방안**

1. 데이터 수집
   - 사생활 침해로 위기
   - 동의에서 책임으로 강화해서 통제
2. 데이터 활용
   - 책임원칙 훼손으로 위기
   - 결과 기반 책임 원칙 고수해서 통제
3. 데이터 처리
   - 데이터 오용으로 위기
   - 알고리즘 접근 허용해서 통제



---



## 3. 데이터 분석 방안 수립



**분석 대상과 방법에 따른 분류**

|                  |         | 분석의 대상(what) |               |
| ---------------- | ------- | ----------------- | ------------- |
|                  |         | known             | unknown       |
| 분석의 방식(how) | known   | **Optimization**  | **Insight**   |
|                  | unknown | **Solution**      | **Discovery** |



**분석 마스터 플랜**

: 분석 과제를 수행함에 있어 그 과제의 목적이나 목표에 따라 전체적인 방향성을 제시하는 기본계획

  수립 시 일반적인 정보전략계획 방법론 활용할 수 있지만, 분석 기획의 특성을 고려해서 수행해야



**정보전략계획(ISP)**

: 정보기술 및 시스템을 전략적으로 활용하기 위한 중장기 마스터 플랜 수립하는 절차



**빅데이터 분석 프로젝트의 우선순위 평가기준**

ROI요소 4V

- Volume(크기)
- Variety(형태)
- Velocity(속도)
- Value(가치) - 비즈니스 효과



**ROI 고려한 과제 우선순위 평가 기준**

시급성(중요)

- 평가 요소 : KPI(핵심성과지표), 전략적 중요도

난이도

- 조직의 상황에 따라 난이도 조율
  - 분석 준비도와 성숙도 진단 결과 활용해서 조직의 분석 수준 파악
  - 적용 범위와 수행 방법별 난이도 조정



**포트폴리오 사분면 분석 기법 활용** - 난이도와 시급성 기준

- 우선순위 기준을 시급성에 둘 경우 순서 : 3 - 4 - 1 - 2
- 난이도에 둘 경우 순서 : 3 - 1 - 4 - 2

> 근데 나는 여기 둘이 반대라고 생각하는데 왜 책에는 이렇게 쓰여있는거지..?
>
> 다른 곳들 찾아봐도 좀 다르고 그 사람들도 이상하다는데..;



**분석 로드맵** - 마스터플랜에서 정의한 목표를 기반으로 분석 과제를 수행하기 위해 핗ㄹ요한 기준 등 담아 만든 종합적 계획

- 절차
  1. 데이터 분석체계 도입
  2. 데이터 분석 유효성 검증
  3. 데이터 분석 확산 및 고도화



**분석 과제 도출 방법**

- 최적의 의사결정을 위한 혼합방식
  - 상향식 발산 단계 : 가능한 옵션 도출
  - 하향식 수렴 단계 : 도출된 옵션 분석하고 검증

1. 하향식(TOP-DOWN)

   - 단계

     1. 문제 탐색 단계
        - 문제 단순 나열이 아닌, 전체적 관점의 기준 모델 활용해서 누락없이 문제 도출하고 식별해야

     2. 문제 정의 단계

     3. 해결방안 탐색 단계

     4. 타당성 평가 단계

   - 문제 탐색 방법
     - 분석 기회 발굴의 범위 확장
       - 거시적 관점
       - 경쟁자 확대 관점
       - 시장의 니즈 탐색 관점
       - 역량의 재해석 관점

2. 상향식(BOTTOM-UP)

   - 전통적 분석 사고 극복방안

     - 디자인 사고 접근법
       - WHY를 강조하기 보다, 사물을 있는 그대로 인식하는 WHAT관점으로 접근

   - 문제 해결 방법

     - 프로토타이핑 접근법 : 분석 먼저 해보고 결과 확인하면서 반복적으로 개선
       1. 가설 새성
       2. 디자인에 대한 실험
       3. 실제 환경에서 테스트
       4. 통찰 도출 및 가설 확인

     

**데이터 분석 방법론**

생성과정(선순환 과정)

- 암묵지 > (형식화) > 형식지 > (체계화) > 방법론 > (내재화) > 암묵지 > ...
- 형식화
  - 개인의 암묵지 > 조직의 형식지
  - 분석가 경험 바탕으로 정리해서 문서화
- 체계화
  - 절차, 활동, 작업, 산출물, 도구 정의
- 내재화
  - 개인에게 전파되고 활용 > 암묵지로 발전
  - 방법론 학습, 활용 > 내재화



**계층적 프로세스 모델 구성**

1. 단계(phase) - 최상위 계층
   - 완성된 단계별 산출물 생성
2. 태스크(task) - 중간 계층
3. 스텝(step) - 최하위 계층
   - WBS(work breakdown structrue)의 워크패키지
   - 입력자료, 처리 및 도구, 출력자료로 구성 > 단위 프로세스



**소프트웨어 개발생명주기의 구성요소**

1. 폭포수 모형

2. 프로토타입 모형

   - 일부분을 일시적으로 간략히 구현 > 다시 요구사항 반영하는 과정 반복

   - 실험적 프로토타입 : 실제 개발될 sw의 일부분을 직접 개발 > 의사소통 도구로

   - 진화적 프로토타입 : 도구로만 활용하지 않고, 이미 개발된 프로토타입을 지속적으로 발전시켜 > 최종 소프트웨어로 발전

3. 나선형 모형

   - 점진적으로 완벽한 시스템으로 개발

4. 반복적 모형 : 시스템을 여러 번 나눠서 릴리즈

   - 증분형 모형
     - 요구사항과 제품의 일부분을 반복적으로 개발 > 대상범위를 확대
   - 진화형 모형
     - 구성요소의 핵심부분을 개발 > 각 구성요소 발전



**KDD 분석 방법론(knowledge discovery in database)**

: Fayyad가 통계적인 패턴이나 지식 탐색하는데 활용할 수 있도록 체계적으로 정리한 프로파일링 기술 기반 데이터 마이닝 프로세스

1. 비즈니스 도메인 이해
2. 데이터셋 선택과 생성
3. 잡음과 이상값 제거(선처리)
4. 변수 찾고, 차원 축소하는 데이터 변경
5. 데이터 마이닝 기법 선택
6. 데이터 마이닝 알고리즘 선택
7. 데이터 마이닝
8. 결과 해석
9. 발견된 지식 활용

- 크게 데이터셋 선택 > 전처리 > 변환 > 마이닝 > 평가 5단계



**CRISP-DM 분석 방법론(cross industry standard process for data mining)**

: 계층적 프로세스 모델

1. 최상위 레벨
   - 여러 단계로 구성
2. 일반화 태스크
   - 단일 프로세스 완전하게 수행
3. 세분화 태스크
   - 일반화 태스크 구체적으로 수행
4. 프로세스 실행
   - 데이터마이닝 구체적으로 실행

- 업무 이해 > 데이터 이해 > 데이터 준비 > 모델링 > 평가 > 전개



| CRISP-DM               | KDD                         |
| ---------------------- | --------------------------- |
| business understanding |                             |
| data understanding     | selection + preprocessing   |
| data preparation       | transformation              |
| modeling               | data mining                 |
| evaluation             | interpretation / evaluation |
| deployment(전개)       |                             |



**SENMA 분석 방법론**

: Sample, Explore, Modify, Model and Assess

  기술과 통계 중심의 데이터 마이닝 프로세스

1. 추출(sample)
2. 탐색(explore)
3. 수정(modify)
4. 모델링(model)
5. 평가(assess)



**빅데이터 분석 방법론**

: 응용 서비스 개발 위한 3계층

1. 단계
   - 기준선 설정하고 버전관리해서 통제해야
2. 태스크
   - 세부 업무
3. 스텝
   - 단기간 내 수행 가능한 워크패키지

- 개발 절차 : 분석 기획 > 데이터 준비 > 데이터 분석 > 시스템 구현 > 평가 및 전개

분석 기획

- 비즈니스 이해, 범위 설정, 프로젝트 정의, 계획 수립(WBS 작성), 위험계획 수립(회피, 전이, 완화, 수용)

데이터 준비

- 필요 데이터 정의, 데이터 스토어 설계(정형, 비정형, 반정형 모두 저장 가능하도록), 수집, 정합성 점검(ETL도구 활용)
- 정형 데이터 스토어는 일반적으로 관계형 db, 비정형 반정형 데이터 스토어는 하둡, NoSQL 사용

데이터 분석

- 분석용 데이터 준비, 텍스트 분석, 탐색적 분석, 모델링, 평가 및 검증

시스템 구현

- 시스템 및 데이터 아키텍쳐와 사용자 인터페이스 설계

평가 및 전개

- 프로젝트 성과를 정량적, 정성적으로 나눠서 평가서 작성



**데이터 분석 거버넌스**

> 데이터 분석 업무를 하나의 기업 문화로 정착하고, 지속적으로 고도화 해나가기 위해 필요

데이터 분석 과제 관리 프로세스

- 분석 idea 발굴 > 분석과제 후보 제안 > 분석과제 확정 > 팀 구성 > 과제 실행 > 진행 관리 > 결과 공유/개선

데이터 분석 지원 인프라

- 단기적으로 구축하기 쉬운 개별 시스템보다는, 확장성 고려한 플랫폼 구조 도입이 적절(버스형처럼 생김)
  - 데이터 분석 플랫폼
    - 응용프로그램이 실행될 수 있는 환경과 기초 이루는 컴퓨터 시스템
    - 중앙집중적 데이터 관리
    - 시스템 간 인터페이스 최소화
    - 새로운 분석 니즈 발생할 때 개별 시스템 추가안해도 추가적인 서비스 제공 가능(확장성 증대)
- 구성 요소
  - 조직(organization)
  - 데이터 분석 과제 기획, 운영 프로세스(process)
  - 데이터 분석 지원 인프라(system)
  - 데이터 거버넌스(data)
  - 데이터 분석 교육, 마인드 육성 체계(human resource)

데이터 거버넌스

> 개별 시스템 단위로 관리하면 중복, 비표준화에 따라 활용도 저하될 수 있음
>
> 단발성을 줄이기 위해

- 전사 차원의 모든 데이터에 대해 정책, 지침, 표준화, 운영조직, 책임 등의 표준화된 관리 체계 수립하고 운영하기 위한 프레임워크와 저장소 구축
- 주요 관리 대상
  - 마스터 데이터
    - 마스터 파일 형성하는 데이터, 데이터 처리 및 조작하기 위해 사용되는 기본 데이터
  - 메타 데이터
    - 데이터에 대한 구조화된 데이터, 다른 데이터 설명 위해 사용되는 데이터
  - 데이터 사전
    - 다른 자료와의 관계 등 저장해놓은 데이터
- 가용성, 유용성, 통합성, 보안성, 안전성 확보
- 독자적 구축도 가능하지만, 전사 차원의 it거버넌스나 EA의 구성요소가 될 수 있음
- 구성 요소
  - 원칙
    - 데이터 유지하고 관리하기 위한 지침 가이드
    - 보안, 품질기준, 변경관리
  - 조직
    - 조직의 역할과 책임
    - 데이터 관리자, db 관리자, 데이터 아키텍트
  - 프로세스
    - 관리 위한 활동과 체계
    - 작업 절차, 모니터링 활동, 측정 활동
- 체계
  - 데이터 표준화
    - 명명 규칙 수립(mapping 유지)
    - 메타 데이터, 데이터 사전 구축
  - 데이터 관리 체계
  - 데이터 저장소 관리
    - 데이터 관리 체계 지원 위한 워크플로우, 관리용 어플리케이션 지원해야
  - 표준화 활동



**데이터 분석 문화 도입방안**

- 문화적 대응
  - 분석 내재화 단계 : 준비기 > 도입기 > 안정 추진기
- 적극적 도입방안
  - 적합한 데이터 분석 과제 도출
  - 데이터 분석 조직 마련
- 데이터 분석 교육방향
  - 데이터 분석 큐레이션 교육 진행
  - 분석 기회 발굴, 시나리오 작성법 교육
  - 분석적 사고 향상 교육



**데이터 분석 수준진단**

> 6개 분석 준비도, 3개 분석 성숙도 동시에 평가

- 분석 준비도(readiness)
  - 조직 내 데이터 분석 업무 도입을 목적으로 현재 수준 파악하기 위한 진단방법
  - 분석 업무, 분석 인력 조직, 분석 기법, 분석 데이터, 분석 문화, 분석 인프라
- 분석 성숙도
  - 데이터 분석 능력 및 결과 활용에 대한 조직의 성숙도 수준 평가
  - 비즈니스, 조직 역량, it >> 도입 > 활용 > 확산 > 최적화

분석 수준진단 결과

1. 분석 준비도 및 성숙도 진단 결과

2. 사분면 분석

   | 정착형 | 확산형 |
   | ------ | ------ |
   | 준비형 | 도입형 |

   <세로축 : 성숙도>																<가로축 : 준비도>



---



## 4. 분석 작업 계획

> 데이터 처리 영역과 데이터 분석 영역으로 나눌 수 있음



**데이터 처리 영역**

> 기초 데이터 정의, 수집 및 저장, 분석하기 수월하도록 물리적인 환경 제공하는 영역

- 데이터 소스 / 데이터 수집 / 데이터 저장 / 데이터 처리



**데이터 분석 영역**

> 데이터 추출해서 분석 목적과 방법에 맞게 가공, 분석 수행, 결과 표현

- 도메인 이슈 도출 / 분석목표 수립 / 프로젝트 계획 수립 / 보유 데이터 자산 확인



**데이터 확보 계획**

- 분석 변수 생성 프로세스 정의
  - 객관적 사실 기반 문제 접근
  - 데이터 상관분석
  - 프로토타입 통한 분석 변수 접근
- 생성된 분석 변수의 정제 위한 점검항목
  - 데이터 수집
    - 적정성
    - 가용성 - 수집 가능한 데이터인가
    - 대체 분석 데이터 유무
  - 데이터 적합성
    - 데이터 중복
    - 변수별 범위
    - 변수별 연관성
    - 내구성 - 노이즈, 왜곡 발생시 예측 성능을 보장할 수 있는가
  - 특징 변수
    - 특징 변수 사용 - 분석 변수 중 바로 특징 변수로 사용할 수 있는 가능성 유무
    - 변수 간 결합 가능 여부 - 분석 변수 결합해서 교차 검증 가능성 유무
  - 타당성
    - 편익/비용 검증
    - 기술적 타당성
- 생성된 분석 변수의 전처리 방법 수립
  - 정제(cleaning)
  - 통합(integration)
  - 축소(reduction)
  - 변환(transformation)
  - 데이터 전처리 과정은 정제와 통합을 통해 60 ~ 80% 처리됨
- 생성 변수의 검증 방안 수립