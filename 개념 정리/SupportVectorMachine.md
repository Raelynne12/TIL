# SVM(서포트벡터머신)

> 클래스를 잘 분류할 수 있는 최적의 벡터 선분을 찾는 알고리즘
>
> 최적의 벡터 선분 >>  초평면(hyper plane)
>
> 지지선(지지 벡터_서포트 벡터)
>
> 경계면(margin)을 최대로 하는 지지선을 찾는 것



hyper plane보다 위면 class1, 아래면  class2 (멀티클래스도 가능(다중 분류))



비선형 경계면 분류 > 선형으로 변환해야 함 > 매핑함수



매핑함수 사용시

- 데이터 > 선형적 형태로 변형
- 축 > 매우 복잡한 형태로 변형
- 차원의 저주가 발생할 수 있음
  - representer 정리 활용
    - 임의의 단조 증가 함수와 비음수 손실함수 활용



비선형 svm

- 적절한 카파(커널함수) 결정
- 비선형과 고차원 데이터에서 비선형 결정경계를 갖는 모형 생성
- 의사결정나무는 엔트로피에서 데이터를 선형으로 구분 
  - 데이터가 비선형일 경우 오분류 다수 발생
- 해결 방법 > 비선형 svm



차원축소

- 차원의 저주 피하기 위해
- pca(주성분 분석)
  - 많은 변수로 구성된 데이터에 주성분이라는 새로운 변수 생성해서 기존 변수들보다 차원 축소해서 분석 수행
  - **새로운 축 설계**
  - 정보 최대로 유지하면서 관련성 분석해서 해석 가능한 적은 개수의 새로운 변수들로 축소
  - 데이터 셋에서 feature 별로 표준화(평균 빼고 분산 나누기)
  - feature data의 공분산 행렬 구하기
  - 공분산 행렬의 **고유값(분산의 크기), 고유 벡터(분산 방향)** 구하기
  - 고유값이 큰 순서대로 내림차순 정렬
  - d차원 줄이고 싶을 때 : 크기 순서대로 d개 고유값 선정
  - 고유벡터로 새로운 행렬 생성
  - 원본 데이터를 새로운 행렬에 투영(projection)



---

# 앙상블





- 과적합 가능성 줄어듬
- 평균을 취해서 편의를 제거
- 다수의 훈련자료 생성
- 배깅
  - 같은 알고리즘 사용, 훈련용 데이터 셋을 무작위로 구성해서 다르게 학습
  - 데이터가 균등할 때 성능이 좋음
- voting
- 랜덤포레스트
  - 인기 모델
  - 회귀분석/군집분석에 응용
  - 노이즈에 강함
  - 연속형 데이터도 예측 가능
  - 여러 표본이 있어야 해서 학습데이터가 충분히 있어야 함
  - 안그러면 오버피팅 위험 생김
- 부스팅
  - 예측력 약한 모형들 결합해서 강한 예측 모형 만들기
  - 오류 데ㅣㅇ터에 대해 더 세심한 배려
  - 데이터 안좋은 데이터에 가중치 둠
  - 훈련 오차를 빨리. 쉽게 줄일 수 있음
  - 배깅보다 예측오차 줄어들어서 성능 향상
  - adaboost > xgboost > lightgbm
  - 경계선에 애매하게 있으면 더 많은 과ㄴ심 가지고 집중 훈련
  - 그래서 시간이 오래 걸림
  - 모델 성능 고려한 가중치 투표
  - 부스팅은 약한 모델(배깅은 제일 좋은 거 , 베스트)
- 데이터 골고루 사용하기 때문에 노이즈에 강한 모형 만들어지
- 학습데이터 적으면 과적합이 굉장히 커짐



---

# 시계열 자료

- 추세 요인
  - 장기간 걸친 상승이나 하강의 경향, 
  - 전반적으로 주기없이 데이터 성장하거ㅏㄴ 하락하는 경우(ex 키)
- 계절성 요인
  - 특정 기간 동안의 주기적 변동성(평일과 휴일이 만드시 차이가 있음)
- 순환 요인
  - 계절성 이외의 요인으로 인한 추세 이탈
- 불규칙한 요인

- 회귀분석의 확장 느낌

비정상시계열 > 정상시계열 전환방법



- 자기 상관계수
- 교차 상관계수



- 자기상관함수(ACF)	
- 편자기상관함수(PACF)



- 자기상관모델

  - 어떤 변수의 과거가 미래의 자신의 변수에 영향을 미치는 관계	

  - 시계얄이 실제 과거값의 함수로 표현

  - 검사 위해 PACF 사용

- 이동평균모델(MA)



- 정상성
  - 모든 시점 t에 대해 평균 일정
  - 분산은 시점 t에 의존하지 않음
  - 두 시점 t,s가 있다면 공분산 cov(Zt,Zs)는 시차에 의존(t-s)하고, t,s에 의존하진 않는다