# SVM(서포트벡터머신)

> 클래스를 잘 분류할 수 있는 최적의 벡터 선분을 찾는 알고리즘
>
> 최적의 벡터 선분 >>  초평면(hyper plane)
>
> 지지선(지지 벡터_서포트 벡터)
>
> 경계면(margin)을 최대로 하는 지지선을 찾는 것



hyper plane보다 위면 class1, 아래면  class2 (멀티클래스도 가능(다중 분류))



비선형 경계면 분류 > 선형으로 변환해야 함 > 매핑함수



매핑함수 사용시

- 데이터 > 선형적 형태로 변형
- 축 > 매우 복잡한 형태로 변형
- 차원의 저주가 발생할 수 있음
  - representer 정리 활용
    - 임의의 단조 증가 함수와 비음수 손실함수 활용



비선형 svm

- 적절한 카파(커널함수) 결정
- 비선형과 고차원 데이터에서 비선형 결정경계를 갖는 모형 생성
- 의사결정나무는 엔트로피에서 데이터를 선형으로 구분 
  - 데이터가 비선형일 경우 오분류 다수 발생
- 해결 방법 > 비선형 svm



차원축소

- 차원의 저주 피하기 위해
- pca(주성분 분석)
  - 많은 변수로 구성된 데이터에 주성분이라는 새로운 변수 생성해서 기존 변수들보다 차원 축소해서 분석 수행
  - **새로운 축 설계**
  - 정보 최대로 유지하면서 관련성 분석해서 해석 가능한 적은 개수의 새로운 변수들로 축소
  - 데이터 셋에서 feature 별로 표준화(평균 빼고 분산 나누기)
  - feature data의 공분산 행렬 구하기
  - 공분산 행렬의 **고유값(분산의 크기), 고유 벡터(분산 방향)** 구하기
  - 고유값이 큰 순서대로 내림차순 정렬
  - d차원 줄이고 싶을 때 : 크기 순서대로 d개 고유값 선정
  - 고유벡터로 새로운 행렬 생성
  - 원본 데이터를 새로운 행렬에 투영(projection)



---

# 앙상블





- 과적합 가능성 줄어듬
- 평균을 취해서 편의를 제거
- 다수의 훈련자료 생성
- 배깅
  - 같은 알고리즘 사용, 훈련용 데이터 셋을 무작위로 구성해서 다르게 학습
  - 데이터가 균등할 때 성능이 좋음
- voting
- 랜덤포레스트
  - 인기 모델
  - 회귀분석/군집분석에 응용
  - 노이즈에 강함
  - 연속형 데이터도 예측 가능
  - 여러 표본이 있어야 해서 학습데이터가 충분히 있어야 함
  - 안그러면 오버피팅 위험 생김
- 부스팅
  - 예측력 약한 모형들 결합해서 강한 예측 모형 만들기
  - 오류 데ㅣㅇ터에 대해 더 세심한 배려
  - 데이터 안좋은 데이터에 가중치 둠
  - 훈련 오차를 빨리. 쉽게 줄일 수 있음
  - 배깅보다 예측오차 줄어들어서 성능 향상
  - adaboost > xgboost > lightgbm
  - 경계선에 애매하게 있으면 더 많은 과ㄴ심 가지고 집중 훈련
  - 그래서 시간이 오래 걸림
  - 모델 성능 고려한 가중치 투표
  - 부스팅은 약한 모델(배깅은 제일 좋은 거 , 베스트)
- 데이터 골고루 사용하기 때문에 노이즈에 강한 모형 만들어지
- 학습데이터 적으면 과적합이 굉장히 커짐



---

# 시계열 자료

- 추세 요인
  - 장기간 걸친 상승이나 하강의 경향, 
  - 전반적으로 주기없이 데이터 성장하거ㅏㄴ 하락하는 경우(ex 키)
- 계절성 요인
  - 특정 기간 동안의 주기적 변동성(평일과 휴일이 만드시 차이가 있음)
- 순환 요인
  - 계절성 이외의 요인으로 인한 추세 이탈
- 불규칙한 요인

- 회귀분석의 확장 느낌

비정상시계열 > 정상시계열 전환방법



- 자기 상관계수
- 교차 상관계수



- 자기상관함수(ACF)	
- 편자기상관함수(PACF)



- 자기상관모델

  - 어떤 변수의 과거가 미래의 자신의 변수에 영향을 미치는 관계	

  - 시계얄이 실제 과거값의 함수로 표현

  - 검사 위해 PACF 사용

- 이동평균모델(MA)



- 정상성
  - 모든 시점 t에 대해 평균 일정
  - 분산은 시점 t에 의존하지 않음
  - 두 시점 t,s가 있다면 공분산 cov(Zt,Zs)는 시차에 의존(t-s)하고, t,s에 의존하진 않는다



---

---

---

# 데이터 마이닝

> 대용량 데이터에서 의미있는 데이터 패턴을 파악하거나
>
> 예측을 위해 데이터를 자동으로 분석해서 의사결정에 활용



\>> 변수를 줄여가는 게 목적



**통계 vs 데이터마이닝**

통계

- 가설을 명제형태로 만들어서 세우고(영가설/귀무가설), 대립가설(연구가설) 차이가 있다고 하는 게 목적
- 보수적 관점(하나씩 디테일하게)

데이터 마이닝

- 가장 의미 있는 걸 뽑는 거(pca분석/유인분석)
- 서로 비슷한 것들끼리 묶어서

train data | test data (0.7:0.3)

fine tuning(미세 조정) > 로버스트(튼튼하게 만듦) >> 모형접근법



**개요**

\- 회귀분석 / 로짓분석 / 최근접 이웃(KNN) / 인공지능 / 의사결정나무(직관적/이분법적, 가지치기) / k-평균군집화(k>하이퍼파라미터) / 연관분석(if~then)



**특징**

- 분석 대상이나 활용 목적, 표현 방법에 따라
  - 시각화 분석, 분류, 군집화, ...





**학습법**

- 지도학습
  - supervised learning
  - label 
  - 정답을 알려주면서 학습
  - 출력변수 있음
- 비지도학습
  - unsupervised learning
  - 군집분석
  - 정답 안알려주고 비슷한 데이터 군집화 > 미래 예측
  - 출력변수 없음



**추진단계**

1. 목적정의
2. 데이터 준비
   - 데이터가 충분한가(나중에 프로젝트할 때 api나 공공데이터를 이용해서 해야)
   - 관심사가 비슷한가
3. 데이터 가공(중요)
4. 데이터마이닝 기법 적용(model)(모델을 목적에 맞게 선택)
5. 검증(성능평가)

\- 평가하는 사람들은 4, 5번 봄(가공 잘 못하면 검증에서 성능 안좋게 나옴)



**데이터 분할**

- 훈련용(training)
  - 초기 데이터마이닝 모델 만드는데 사용 (추정용, 훈련용)

- 검정용(validation)
  - 구축된 모델의 과잉(overfitting) 또는 과소맞춤 등의 미세조정 절차를 위해
- 시험용(test)
  - 데이터마이닝 추진 5단계에서 검증용으로 사용

\- 실제로는 train/test(hold out절대 만지면 안돼) 이렇게 두 갠데 kaggle에서 세 개로 세분화(검정용)

\- 데이터셋이 작을 경우 > 필요에 따라 훈련용과 시험용을 번갈아가며 사용

   \>> 교차확인(cross )을 통해서 



**모형평가**

\- 몇 가지 모형 중 어느 것이 적합한지 판단하는 기준 >> 손익비교

\- 모델링은 변경 주기 O, 정확도의 편차가 급증하는 시점에 실행(최소 1년 두 번)



**분류분석**

- 목적 : 반응변수(종속변수)가 알려진 다변량 자료를 이용해서 모형 구축 > 새로운 자료에 대한 예측 및 분류
- 모형 : 로지스틱회귀모형, 신경망모형, 의사결정나무모형, 앙상블모형(케글에서 많이 씀), 규칙기반분류, 사례기반분류, 인접이웃분류모형, 베이즈분류모형, 유전자 알고리즘



**분류 vs 예측**





**의사결정나무**

\- 뿌리 마디 / 부모 마디 / 자식 마디 / 최종마디(끝마디) / 중간 마디 / 가지 / 깊이

\- 결정 규칙

- 분할 규칙
  - 새 가지를 어디에서 나오게 할까
- 정지 규칙
  - 어떤 기준을 만족하면 새 가지가 더 이상 못나오게 하는가
- 가지치기 규칙
  - 어느 가지를 쳐내야 예측력 높은 나무가 되는가

\- 특징

1. 설명 용이
2. 시각화해서 한 눈에 볼 수 있음
3. 계산적으로 복잡하지 않아서 대용량 데이터에서도 빠르게 만들 수 있음
4. 분류 작업 빠르게
5. 잡음 데이터에 대해서도 민감함없이 분류 가능
6. 정확도 높음
7. 비모수적 모형(비모수 통계)
8. 수치형/범주형 변수를 모두 사용할 수 있음
9. 너무 깊이가 깊어질수록  과적합



\- 지니 계수(gini index) : 특정 클래스에 속하는 데이터의 비율을 모두 제외한 값 >> 다양성을 계산하는 방법

​										  yes확률 제곱 - no확률 제곱

지니 불순도(gini impurity)    <<  **계산하는 거 시험에 낼거래**

\>> 가장 불순도가 낮게 나오는 기준을 선택

깊이가 깊어질수록 세분화해서 나눌 수 있지만 과적합 가능성 있음



암이 아닌데 암 > 2형 오류

**1형 오류 2형 오류 이거 시험나옴**



\- 엔트로피  : 불순도를 수치화한 지표 중 하나 > 불확실성을 수치로 나타낸 것

1. 엔트로피 수치가 0이면 > 분류할 필요없음(이물질이 없음)
2. 엔트로피 수치가 1이면 > 불순도 높다
3. 엔트로피 수치가 0에 가까우면 > 불순도가 낮다

이 엔트로피를 이용해서 정보획득량(information gain)을 구하게 됨

\- 정보획득량 = 분할 전 엔트로피 - 분할 후 엔트로피

정보획득량이 크면 > 불순도가 줄어든다



\- rpart 패키지 : CART(classification and regression trees) 방법론을 사용해서 의사결정나무 구현



\- party패키지 : p-value 이용해서 가지를 만들기 때문에 가지치기할 필요 없음



**오분류표** : 목표 변수의 실제 범주와 모형에 의해 예측된 분류 범주 사이의 관계를 나타내는 표

\- TP/TN/FP/FN

- 정밀도
  - precision
  - TP
  - negative가 중요한 경우
    - 일반 메일을 스팸 메일로 잘못 예측했을 경우 중요한 메일을 전달받지 못하는 상황이 생길 수 있음
- 정확도
  - accuracy
  - TP+TN
- 재현율 = 민감도
  - recall = sensitivity
  - TP
  - positive가 중요한 경우
    - 악성 종양을 양성 종양으로 잘못 예측했을 경우 제 때 치료를 받지 못하게 되어 생명이 위급해질 수 있음
- 특이도
  - specificity
  - TN

\- 오차행렬(confusion matrix) : 데이터 실제 클래스와 모델에 의해 예측된 클래스를 비교하는 행렬

​												      **F1-score**

(여기 좀 더 채워야 함)



혼동 행렬 > 분류 결과를 전체적으로

정확도 >  정답을 얼마나 잘 맞췄는지

정밀도, 재현율 >  

(여기도 채워)



---



### 로지스틱 회귀분석



1. 모델을 이용한 예측 및 결과값 출력
2. 예측 결과와 실제 결과값 비교
3. 예측 정확도 계산 및 출력



시그모이드 : 이진분류

소프트맥스 : 다중분류



**시그모이드** : s자형 곡선 or 시그모이드 곡선을 갖는 수학 함수 / 가역 함수(역은 로짓함수)

-특징

- 실함수로써 미분 가능
- 음이 아닌 미분값을 가지고, 단 하나의 변곡점 가짐
- 일반적으로 단조함수
- 종 모양의 1차 미분 그래프
- x가 무한으로 갈 때 한 쌍의 수평 점근선으로 수렴
- 0보다 작은 값에서 볼록하고, 0보다 큰 값에서 오목



**소프트맥스**(nomalized expotential function) : 

\- 특징

- 시그모이드 함수의 일반화
- 확률이론이나 확률적 머신이론에 활용

(여기도 채워)



---



### 앙상블

> 여러 개 분류모형에 의한 결과를 종합해서 분류의 정확도 높임



**특징**

- 과적합 가능성 줄여줌
- 평균 취함으로써 편의 제거
- 변동성 작아짐(한 개 모형으로부터 여러 모형의 의견을 결합하면)



**생긴 이유**

- 나무 모형의 불안정성
- 분기변수 선택 문제
- 분기변수 변화에 따라 모형이 크게 달라짐(불안정성 야기)



**기본 형태**

- 부트스트랩 표본 추출로 다수의 훈련자료 생성
- 각 훈련자료에 대해 동일한 알고리즘으로 모형 생성
- 결과 결합해서 최종 예측치 산출



**용어 설명**

1. 배깅(bagging)
   - 여러 개 부트스트랩 자료 생성
   - 각 부트스트랩 자료에서 예측 모형 만든 후 결합 > 최종 예측 모형 
   - 랜덤 포레스트(random forest)
     - 임의성
     - 예측력 향상
     - 투표 방법(voting) : 규칙이나 조건문을 토대로 나무 구조를 도표화해서 분류, 예측
     - classification tree, regression tree
2. 부트스트랩(bootstrap)
   - 동일한 크기의 표본은 무작위 복원추출(난수)로 뽑은 자료
   - 유사한 훈련용 자료를 반복해서 재선정한다는 점에서 차이가 있음
   - 양이 크지 않을 때한다
3. 부스팅(boosting)
   - 예측력 약한 모형들 결합 > 강한 예측 모형 만들기
   - 훈련 오차를 빨리, 쉽게 줄일 수 있음
   - 예측오차가 적어져서 성능 향상



(hyper parameter > 적절한 튜닝)



**분류 분석 모형 평가** >> **2종오류**

- 교차검증(cross validation)
  - k-fold 교차검증이 대표적
    - 데이터를 사이즈 동일한 k개 하부집합으로 나누고 k번째 하부집합을 검증용 자료로, 나머지 하나를 훈련용 자료로 사용



---



### 성과분석, 인공신경망분석



**성과분석**

- 홀드아웃
  - 원본 데이터를 랜덤하게 두 분류로 분리하여 교차 검증하는 방법
  - 7:3
- 교차검증
  - 주어진 데이터를 가지고 반복적으로 성과를 측정해서 결과를 평균 > 분류 분석 모형을 평가하는 방법
  - 대표적 k-fold



**특징**

- roc curve
  - 분류분석 결과의 성과분석을 위한 그림
  - 민감도(y축), 1-특이도(x축)를 할용해서 평가
  - 둘 다 1일 땐 true, 둘 다 0이면 false
  - 밑 면적은 auc
  - 밑 면적이 0.85다 >> 85%
- 이익도표(lift chart)
  - 이익 : 목표 범주에 속하는 개체들이 각 등급에 얼마나 분포하고 있는지
  - 이익 도표 : 해당 등급에 따라 계산된 이익 값을 누적으로 연결
  - \>> 각 등급이 얼마나 포함되는지
- 향상도 곡선
  - 랜덤 모델과 비교해서 성과가 얼마나 향상되었는지 등급별 파악
  - 상위 등급 : 향상도 매우 큼 > 예측력 적절
  - 향상도(lift) = 반응률  / 기본 향상도
  - 좋은 모델이라면 lift가 빠른 속도로 감소해야 함



**인공신경망분석**

- ann(artificial neural network) (perceptron)

  - 신호의 교환으로 정보를 저장하고 학습
  - 시냅스 결합으로 네트워크를 형성한 신경세포가 학습을 통해 시냅스의 겷랍 세기를 변화시켜 문제를 해결하는 모델

- 고려사항

  - 입력변수
    - 복잡하기 때문에 입력 자료 선택에 매우 민가
    - 입력변수가 범주형 > 모든 범주에서 일정 빈도 이상 값 갖고 빈도가 일정할 때
    - 입력변수가 연속형 > 값들의 범위가 변수 간 큰 차이가 없을 때 신경망 모형에 적합
  - 가중치 초기값과 다중 최소값 문제
    - 역전파 알고리즘은 초기값에 따라 결과값 많이 달라짐
    - 가중치 0이면 시그모이드 함수는 선형되고, 신경망 모형은 근사적으로 선형모형됨
    - 초기값은 0근처로 랜덤하게 선택하므로 초기 모형은 선형모형에 가깝고, 가중치 값 증가할수록 비선형모형

- 구조

  - 입력층(input layer)
    - 각각 입력변수가 1:1로 매칭되는 뉴런 존재
  - 은닉층(hidden layer) - 비선형(non-linear)
    - 2개 이상 있으면 deep neural network or deep learning 이라고 함
    - 입력층 뉴런과 가중치의 결합으로 생성되는 뉴런 존재
    - 은닉층 개수에 따라 모형 복잡도 결정
  - 출력층(output layer)
    - 은닉층에서 뉴런과 가중치가 결합하여 생성되는 뉴런 존재
    - 예측하고자 하는 종속변수 형태(numeric)에 따라 출력층 개수가 결정
  - 히든충과 출력층이 존재하는 뉴런
    - 이전 층에서 입력값, 가중치를 합
    - 뉴런 가중합을 입력값으로 신호를 출력하는 활성화 함수기능 수행(sigmoid, softmax 등)

  **(시그모이드 소프트맥스 다중분류 이진분류 이거 중요해)**

  

---



### 군집분석

> 대표적인 비지도학습
>
> 유사한 성격으로 군집화(거리 기준)하고 다변량 군집들 사이 관계 분석하는 다변량 분석기법



**특징**

- 별도의 반응변수 노필요
- 유사성만 기초해서 군집 형성
- 이상값 탐지에 사용
- 계층적 군집, 분리 군집, 밀도-기반 군집, 모형-기반 군집, 격지-기반 군집, 커널-기반 군집, som
- 범주형 자료 가능



**계층적 군집**

- n개 군집에서 시작해서 점차 개수 줄여나가는 방법
- 가장 유사한 개체를 묶어나가는 과정 반복해서 원하는 개수 군집 형성
- 결과 형태 > 계통도, 덴드로그램
- 관측벡터 간 거리와 군집 간 거리에 대한 정의 필요
- 군집수 몰라도 분석 가능
- 결과값 이상하면 처음부터 다시 해야 함
- 덴드로그램
  - 군집들 간 구조적 관계 쉽게 볼 수 있음
- 거리 측정 방법에 따라 병합방법 달라짐
  - 중심 연결법
    - 중심 간 거리 
    - 결합될 때 평균은 가중평균
  - 최단 연결법
    - n바이n 행렬에서 거리가 가장 가까운 데이터 묶어서
  - 최장 연결법
    - 거리가 먼 데이터나 군집 묶어서
  - 평균 연결법
    - 거리가 가까운 데이터나 군집 묶어서 형성
    - 평균 사용
  - ward 연결법
    - 군집 내 편차들의 제곱합을 고려
    - 정보 손실 최소화하기 위한 군집화 진행
- r에서 계층적 군집 수행할 때
  - 병합적 방법
  - 분할적 방법

- 연속형 변수일 때

  - 유클리디안 거리(L2) : 데이터간 유사성 측정할 때 많이 사용

  - 맨하탄 거리(L1) : 최단거리 

  - 민코우스키 거리 : 위 두 개 거리를 한 번에 표현한 공식

- 범주형 변수일 때

  - 자카드 거리

  - 자카드 계수

  - 코사인 거리 : 문서를 유사도 기준으로 분류, 그룹핑 할 때

  - 코사인 유사도 : 코사인 값 이용해서 측정된 벡터 간 유사 정도



**비계층적 군집분석**

> 사전에 군집수를 정해줘서 대상이 군집에 할당

- k-means(k-평균) 군집분석
  - k개 평균 찾기
  - 각 군집은 평균값을 대표 
  - 군집수 k에 대해서 군집 내 거리 제곱합의 합을 최소화하는 게 목적
  - 장점
    - 단순하고 빠름
    - 계층 군집보다 많은 양 자료 처리
    - 모든 형태 데이터에 적용 가능
  - 단점
    - 잡음, 이상값에 영향
    - 사전에 군집수 지정
    - k가 원데이터 구조에 적합하지 않으면 좋은 결과 안나옴
  - 센트로이드(centroids)



**혼합분포 군집(가우시안 혼합모형)**

(가우시안하면 정규분포)

- 모형 기반 군집 방법
- 데이터가 k개 모수적 모형(정규분포 or 다변량 정규분포 가정)의 가중합으로 표현되는 모집단 모형으로 나왔다고 가정
- 모수와 함께 가중치를 자료로부터 추정
- k개 모형 > 군집 , k개 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집 분류
- 모수와 가중치 추정에는 em 알고리즘 사용
- 장점
  - 확률분포 도입, 모형기반 군집방법
  - 군집을 몇 개 모수로 표현 가능, 서로 다른 크기나 모양의 군집 찾을 수 있음
- 단점
  - em알고리즘 통한 모수 추정에서 시간 소요
  - 군집 크기 작으면 추정도 저하
  - 이상값에 민감 > 사전에 제거해줘야 함
- em알고리즘
  - e단계(잠재변수 z의 기대치 계산)
    - 각 집단 분포는 정규분포라고 가정
    - 어느 집단에서 나온 자료인지 모르기 때문에 잠재변수 개념 도입
    - 모수 초기값이 주어져있다면, 각 자료가 어느 집단으로부터 나올 확률이 높은지에 대해 추정 가능
  - m단계(z기대치 이용, 파라미터 추정)
    - 조건부 기대값 구함
    - 로그가능도함수에다가 z의 조건부 기대값을 대입해서 로그가능도함수를 최대로 하는 모수를 찾음

- som(자기조직화지도)
  - 비지도 신경망, 저차원의 뉴런으로 정렬 > 지도 형태로 형상화
  - 위치관계 그대로 보존
  - 실제 공간의 변수가 가까이 있으면 지도상에서 가까운 위치에 존재
  - 패턴발견, 이미지 분석에 굿
  - 두개 인공신경망 구조
    - 입력층
      - 입력 벡터를 받는 층
    - 경쟁층
      - 2차원 격자구조로 특성에 따라 한 점으로 클러스터링 되는 층
  - 입력층, 경쟁층, 가중치, 노드
    - 가중치 : 인공신경망에서는 입력값에 대한 중요도값
  - 구조탐색하기 좋음
    - 특징 파악해서 유사 데이터 클러스터링
    - 저차원 2차원으로 표현
  - 차원축소 및 시각화
    - 2차원으로 매핑해서 인간이 시각적으로 인식 가능하게

| 신경망 모형          | som                       |
| -------------------- | ------------------------- |
| 연속적 레이어로 구성 | 2차원 격자(그리드)로 구성 |
| 에러 수정 학습       | 경쟁 학습 실시            |
|                      | **비지도 학습**           |

- 연관분석
  - 항목들 간 조건-결과 식으로 표현 > 패턴, 규칙 찾아냄
  - =  장바구니 분석
  - 측정 지표 **반드시 나옴**
    - 지지도(support)
      - 상품 a와 b 동시에 구매할 확률
      - 동시에 포함된 거래수 / 전체 거래수
    - 신뢰도(confidence)
      - 상품 a가 구매됐을 때 상품 b가 구매될 확률
      - 동시에 포함된 거래수 / a포함하는 거래수
    - 향상도(lift)
      - a 구매한 사람이 b 구매할 확률, a의 구매와 상관없이 b 구매할 확률
      - 동시에 포함된 거래수 / a포함하는 거래수 * b포함하는 거래수
      - 독립이면 > p(a n b) = p(a)p(b) > p(a)p(b)/p(a)p(b) = 1, 즉 향상도는 1
      - 관련성 없으면 lift = 1
      - lift > 1면 관련도 높음
      - lift < 1이면 오히려 a를 구매한 사람은 b를 구매하지 않는다는 결론
  - 절차(apriori 알고리즘 분석 절차)
    1. 최소 지지도 설정
    2. 개별 품목 중에서 최소 지지도 넘는 모든 품목 찾기
    3. 찾은 애를 이용해서 최소 지지도 넘는 두 가지 품목 집합 찾기
    4. 집합을 결합해서 최소 지지도 넘는 세 가지 품목 집합 찾기
    5. 반복해서 최소 지지도 넘는 빈발 품목 찾기
  - 장점
    - 결과 분명
    - 거대 자료 분석 시작으로 적합
    - 계산 용이
  - 단점
    - 품목 수 증가하면 계산량 폭증
    - 속성에 제한(연속형 변수 사용 불가능)
    - 적절한 품목 찾기 어려움
    - 거래 드문 품목에 대한 정보 찾기 어려움